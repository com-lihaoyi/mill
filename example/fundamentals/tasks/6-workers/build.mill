// Mill workers defined using `Task.Worker` are long-lived in-memory objects that
// can persist across multiple evaluations. These are similar to persistent
// tasks in that they let you cache things, but the fact that they let you
// cache the worker object in-memory allows for greater performance and
// flexibility: you are no longer limited to caching only serializable data
// and paying the cost of serializing it to disk every evaluation.
//
// A simple example of a worker is one that maintains a counter in memory,
// incrementing it each time a task that uses the worker is evaluated. Note
// that like normal cached tasks, the task using the worker only re-evaluates
// when its inputs change:

package build
import mill.*, scalalib.*

def input = Task.Source("input.txt")

def counterWorker = Task.Worker {
  new java.util.concurrent.atomic.AtomicInteger(0)
}

def counterTask = Task {
  val count = counterWorker().incrementAndGet()
  println(s"Counting: $count, input: ${os.read(input().path).trim}")
  count
}

/** Usage

> ./mill counterTask
Counting: 1, input: one

> ./mill counterTask # no input change, no re-evaluation

> echo two > input.txt

> ./mill counterTask # input changed, counter increments
Counting: 2, input: two

> echo three > input.txt

> ./mill counterTask
Counting: 3, input: three

> ./mill clean counterWorker

> echo four > input.txt

> ./mill counterTask # cleaning the worker resets the counter
Counting: 1, input: four

*/

// In the above example, the counter only increments when the input file changes.
// Unlike persistent tasks which store state on disk, workers store state in memory.
// `./mill clean` resets the worker by discarding the in-memory object.
//
// Workers live as long as the Mill process. By default, consecutive `mill`
// commands in the same folder will re-use the same Mill process and workers,
// unless `--no-daemon` is passed which will terminate the Mill process and
// workers after every command. Commands run repeatedly using `--watch` will
// also preserve the workers between them.
//
// Common things to put in workers include:
//
// 1. References to third-party daemon processes, e.g. Webpack or wkhtmltopdf,
//    which perform their own in-memory caching
//
// 2. Classloaders containing plugin code, to avoid classpath conflicts while
//    avoiding the cost of launching subprocesses
//
// ==== In-Memory Realistic Example: Compression Cache
//
// This example uses a Worker to provide simple two-level cache: in-memory caching
// for compressed files, in addition to caching them on disk. This means that if the
// Mill process is persistent (e.g. with `--watch`/`-w`) the cache lookups are instant,
// but even if the Mill process is restarted it can load the cache values from disk
// without having to recompute them:

import java.util.Arrays
import java.io.ByteArrayOutputStream
import java.util.zip.GZIPOutputStream

def data = Task.Source("data")

def compressWorker = Task.Worker { new CompressWorker(Task.dest) }

def compressedData = Task {
  println("Evaluating compressedData")
  for (p <- os.list(data().path)) {
    os.write(
      Task.dest / s"${p.last}.gz",
      compressWorker().compress(p.last, os.read.bytes(p))
    )
  }
  os.list(Task.dest).map(PathRef(_))
}

class CompressWorker(dest: os.Path) {
  val cache = collection.mutable.Map.empty[Int, Array[Byte]]
  def compress(name: String, bytes: Array[Byte]): Array[Byte] = {
    val hash = Arrays.hashCode(bytes)
    if (!synchronized(cache.contains(hash))) {
      val cachedPath = dest / hash.toHexString
      if (!os.exists(cachedPath)) {
        println("Compressing: " + name)
        val compressed = compressBytes(bytes)
        synchronized {
          cache(hash) = compressed
          os.write(cachedPath, cache(hash))
        }
      } else {
        println("Cached from disk: " + name)
        synchronized {
          cache(hash) = os.read.bytes(cachedPath)
        }
      }
    } else {
      println("Cached from memory: " + name)
    }
    synchronized { cache(hash) }
  }
}

def compressBytes(input: Array[Byte]) = {
  val bos = new ByteArrayOutputStream(input.length)
  val gzip = new GZIPOutputStream(bos)
  gzip.write(input)
  gzip.close()
  bos.toByteArray
}

//
// NOTE: The _initialization_ of a ``Task.Worker``'s value is single threaded,
// but _usage_ of the worker's value may be done concurrently. The user of
// `Task.Worker` is responsible for ensuring its value is safe to use in a
// multi-threaded environment via techniques like locks, atomics, or concurrent data
// structures. The example above uses `synchronized{}` around all access to the
// shared `cache` state to allow the slow `compressBytes` operation to run in
// parallel while the fast mutation of the `cache` is single-threaded.
//
// Workers can also make use of their `Task.dest` folder as a cache that persists
// when the worker shuts down, as a second layer of caching. The example usage
// below demonstrates how using the `--no-daemon` flag will make the worker
// read from its disk cache, where it would have normally read from its
// in-memory cache

/** Usage

> ./mill show compressedData
Evaluating compressedData
Compressing: hello.txt
Compressing: world.txt
[
  ".../hello.txt.gz",
  "...world.txt.gz"
]

> ./mill compressedData # when no input changes, compressedData does not evaluate at all

> sed -i.bak 's/Hello/HELLO/g' data/hello.txt

> ./mill compressedData # not --no-daemon, we read the data from memory
Compressing: hello.txt
Cached from memory: world.txt

> ./mill compressedData # --no-daemon, we read the data from disk
Compressing: hello.txt
Cached from disk: world.txt

*/

// Mill uses workers to manage long-lived instances of heavyweight objects like the
// https://github.com/sbt/zinc[Zinc Incremental Compiler].
// This lets us keep them in-memory with warm caches and fast incremental execution.
//
// Like any other task, you can use `./mill clean` to wipe out any cached in-memory or
// on-disk state belonging to workers. This may be necessary if your worker implementation
// has bugs that cause the worker disk or in-memory data structures to get into a bad state.
//
// === `Autoclosable` Workers
//
// As <<Workers>> may also hold limited resources, it may be necessary to free up these
// resources once a worker is no longer needed. For example, ``java.net.URLClassLoader``s
// need the `.close()` method to be called, third-party subprocesses spawned by `os.spawn`
// need to `.destroy()` to be called, otherwise these externally-managed resources may leak
// causes your build or system to run out of memory.
//
// To implement resource cleanup, your worker can implement `java.lang.AutoCloseable`.
// Once the worker is no longer needed, Mill will call the `close()` method on it before any newer version of this worker is created.

import mill.*
import java.lang.AutoCloseable

class MyWorker() extends AutoCloseable {
  // ...
  override def close() = { /* cleanup and free resources */ }
}

def myWorker = Task.Worker { new MyWorker() }

// === `CachedFactory` Workers
//
// One very common use case for workers is managing long-lived mutable state. The issue
// with long-lived mutable state is that in the presence of parallelism (the default in
// Mill), managing such state can be tricky:
//
// * If you allow unrestricted access to the mutable state across multiple threads,
//   you are subject to race conditions and non-deterministic bugs
//
// * If you just synchronize/lock all access to the mutable state, you lose all benefits
//   for parallelism
//
// * If you re-generate the mutable state each time for each thread, you lose the benefits
//   of it being long lived
//
// The solution to these issues is to maintain an in-memory cache, with proper locking,
// eviction, and invalidation. Doing so is tedious and error prone, and so Mill provides
// the `CachedFactory` helper to make it easier. The example below re-implements a simplified
// version of `CompressWorker` we saw earlier, but using `CacheFactory` to cache, reset, and
// re-use the `ByteArrayOutputStream` between calls to `cachedCompressWorker().compress`:
//
import mill.*
import java.lang.AutoCloseable
import mill.util.CachedFactory
import java.io.ByteArrayOutputStream

def cachedCompressWorker = Task.Worker { new CachedCompressWorker(Task.dest) }
class CachedCompressWorker(dest: os.Path) // <1>
    extends CachedFactory[Unit, ByteArrayOutputStream] {

  def setup(key: Unit): ByteArrayOutputStream = { // <2>
    println("setup ByteArrayOutputStream")
    new ByteArrayOutputStream()
  }

  def maxCacheSize = 2 // <3>

  def teardown(key: Unit, value: ByteArrayOutputStream): Unit = { // <4>
    println("teardown ByteArrayOutputStream")
    value.reset()
  }

  val cache = collection.mutable.Map.empty[Int, Array[Byte]]
  def compress(name: String, bytes: Array[Byte]): Array[Byte] = {
    val hash = Arrays.hashCode(bytes)
    if (!synchronized(cache.contains(hash))) {
      println("Compressing: " + name)
      val compressed = withValue(()) { bos => compressBytes2(bos, bytes) }
      synchronized { cache(hash) = compressed }
    } else {
      println("Cached from memory: " + name)
    }
    synchronized { cache(hash) }
  }
}

def compressBytes2(bos: ByteArrayOutputStream, input: Array[Byte]) = {
  bos.reset()
  Thread.sleep(1000) // Simulate a slow operation
  val gzip = new GZIPOutputStream(bos)
  gzip.write(input)
  gzip.close()
  bos.toByteArray
}

// <1> `CachedFactory` takes two type parameters, a `K` key type and a `V` value type.
//   In this case `K` is `Unit` since all ``ByteArrayOutputStream``s are the same, but if you
//   are caching things which take some kind of configuration (e.g. compilers with compiler
//   flags) you can set `K` to be the config class so that values with different input
//   configuration are cached separately.
//
// <2> `def setup` creates the `ByteArrayOutputStream` when a new one is necessary
//
// <3> `def maxCacheSize` configures the maximum number of cached entries to keep around while they are not in use,
//
// <4> `def teardown` cleans up the ``ByteArrayOutputStream``s and when the count
//   of cached entries exceeds that `maxCacheSize``
//
// Although this example is synthetic (you don't actually need to reset, reuse, and teardown
// ``ByteArrayOutputStream``s) the same techniques would apply to any long-lived mutable state
// or components you need to manage. This is especially important when
// xref:extending/running-jvm-code.adoc[running JVM code in classloaders or subprocesses],
// as those are both expensive to initialize and need to be properly closed or terminated
// when you are done with them.
//
// The above `cachedCompressWorker` can be used as shown below, with three
// `def compressed*` tasks using it to call `.compress`:

def compressed1 = Task {
  cachedCompressWorker().compress("foo.txt", "I am cow".getBytes)
}

def compressed2 = Task {
  cachedCompressWorker().compress("bar.txt", "Hear me moo".getBytes)
}

def compressed3 = Task {
  cachedCompressWorker().compress("qux.txt", "I weigh twice as much as you".getBytes)
}

/** Usage

> # 3 streams are created on demand, 1 is torn down after due to maxCacheSize = 2 limit
> ./mill show '{compressed1,compressed2,compressed3}'
setup ByteArrayOutputStream
setup ByteArrayOutputStream
setup ByteArrayOutputStream
Compressing: foo.txt
Compressing: bar.txt
Compressing: qux.txt
teardown ByteArrayOutputStream
{
  "compressed1": ...
  "compressed2": ...
  "compressed3": ...
}

> # `clean` clears the CachedFactory and tears down the two streams in the cache
> ./mill clean cachedCompressWorker
teardown ByteArrayOutputStream
teardown ByteArrayOutputStream

*/
