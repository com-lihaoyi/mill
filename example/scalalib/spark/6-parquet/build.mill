package build
import mill.*, scalalib.*

object foo extends ScalaModule {
  def scalaVersion = "2.12.20"
  def mvnDeps = Seq(
    mvn"org.apache.spark::spark-core:3.5.6",
    mvn"org.apache.spark::spark-sql:3.5.6"
  )

  def forkArgs = Seq("--add-opens", "java.base/sun.nio.ch=ALL-UNNAMED")

  object test extends ScalaTests {
    def mvnDeps = Seq(mvn"com.lihaoyi::utest:0.9.1")
    def testFramework = "utest.runner.Framework"

    def forkArgs = Seq("--add-opens", "java.base/sun.nio.ch=ALL-UNNAMED")
  }

}

// ## Parquet Columnar Storage Example
//
// This demonstrates Parquet - the de facto standard storage format for big data analytics.
// While CSV is great for learning, production Spark always uses Parquet (or similar columnar
// formats like ORC). Understanding Parquet is essential for building efficient data pipelines.
//
// ### Why This Example Matters
//
// Parquet dominates production data engineering:
// - **10-100x faster queries** than CSV (columnar vs row-based)
// - **10x smaller files** with compression (Snappy, GZIP, LZO)
// - **Predicate pushdown**: Skip reading irrelevant columns/rows
// - **Schema evolution**: Add/remove columns without rewriting data
// - **Standard format**: Works with Spark, Hive, Presto, Athena, BigQuery
//
// **Real-world adoption:**
// - Netflix processes PBs of Parquet daily
// - Uber's data lake is 100% Parquet
// - AWS Athena queries Parquet 10x faster than CSV
// - Databricks Delta Lake built on Parquet
//
// ### Key Concepts Demonstrated
//
// **1. Columnar vs Row-Based Storage**
// ```
// CSV (row-based): [id,name,age] [id,name,age] [id,name,age]
// Parquet (columnar): [id,id,id] [name,name,name] [age,age,age]
// ```
// - Column queries only read relevant columns (not entire rows)
// - Better compression (similar values stored together)
// - Vectorized processing (SIMD optimization)
//
// **2. Predicate Pushdown**
// ```
// df.filter("age > 30")  // Only reads age column, skips filtered row groups
// ```
// - Parquet stores min/max stats per row group
// - Spark skips row groups where age_max <= 30
// - Massive performance gain on large datasets
//
// **3. Schema Evolution**
// ```
// // Add column without rewriting
// df.withColumn("new_col", lit(null)).write.parquet("path")
// ```
// - Read old files with new schema (missing columns = null)
// - Read new files with old schema (ignore extra columns)
//
// **4. Partitioning for Performance**
// ```
// df.write.partitionBy("year", "month").parquet("path")
// // Creates: path/year=2023/month=01/*.parquet
// ```
// - Queries filter by partition (only read relevant directories)
// - Critical for time-series data
//
// ### Performance Comparison
//
// **CSV:**
// - Read time: 100 seconds (100GB dataset)
// - Storage: 100GB uncompressed
// - Query cost: Full scan every time
//
// **Parquet:**
// - Read time: 10 seconds (10GB compressed)
// - Storage: 10GB with Snappy compression
// - Query cost: Only scans relevant columns + row groups
//
// ### Common Pitfalls
//
// - **Small files**: Many tiny Parquets kill performance (use coalesce/repartition)
// - **Wrong compression**: GZIP = slow writes, Snappy = balanced, Uncompressed = fast
// - **Over-partitioning**: Too many partitions (>10K) causes metadata overhead
// - **Schema mismatch**: Merging schemas requires spark.sql.parquet.mergeSchema=true
//
// ### Next Steps
//
// - Add Delta Lake for ACID transactions
// - Explore Parquet tools (parquet-tools CLI)
// - Learn about column encoding (dictionary, RLE, bit-packing)

/** Usage

> ./mill foo.run
...
CSV read time: 125ms
Parquet write time: 89ms
Parquet read time: 42ms
Performance improvement: 2.97x faster
File size reduction: 65%
...

> ./mill foo.test
...
+ foo.FooTests.parquetReadWrite should preserve data...
...
*/
