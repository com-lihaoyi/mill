package build
import mill.*, scalalib.*

object `package` extends ScalaModule {
  def scalaVersion = "2.12.20"
  def mvnDeps = Seq(
    mvn"org.apache.spark::spark-core:3.5.6",
    mvn"org.apache.spark::spark-sql:3.5.6"
  )

  def forkArgs = Seq("--add-opens", "java.base/sun.nio.ch=ALL-UNNAMED")

  def prependShellScript = ""

  object test extends ScalaTests {
    def mvnDeps = Seq(mvn"com.lihaoyi::utest:0.9.1")
    def testFramework = "utest.runner.Framework"

    def forkArgs = Seq("--add-opens", "java.base/sun.nio.ch=ALL-UNNAMED")
  }

}

// This example demonstrates a more realistic Spark application that reads data
// from a CSV file and computes summary statistics. It also shows how to package
// and deploy the application using `spark-submit`.
//
// === Build Configuration
//
// The build uses `object \`package\`` syntax to define a root-level module,
// making commands shorter (`./mill run` instead of `./mill foo.run`). Key
// configurations:
//
// * `prependShellScript = ""` - Disables the shell script preamble in the
//   assembly JAR. This is required for `spark-submit` compatibility, as Spark
//   expects a plain JAR file without embedded scripts.

/** See Also: src/foo/Foo.scala */

// The application demonstrates several common Spark patterns:
//
// * Reading CSV files with schema inference
// * Using typed Datasets with case classes
// * DataFrame aggregations (`groupBy`, `sum`, `avg`, `count`)
// * Command-line argument handling for input file paths

/** See Also: resources/transactions.csv */

// === Running Locally
//
// During development, you can run the application directly through Mill:

/** Usage

> ./mill run
...
Summary Statistics by Category:
+-----------+------------+--------------+-----------------+
|   category|total_amount|average_amount|transaction_count|
+-----------+------------+--------------+-----------------+
|       Food|        70.5|          23.5|                3|
|Electronics|       375.0|         187.5|                2|
|   Clothing|       120.5|         60.25|                2|
+-----------+------------+--------------+-----------------+
...

> ./mill test
...
+ foo.FooTests.computeSummary should compute correct summary statistics...
...

*/

// === Packaging for Deployment
//
// For production deployment to a Spark cluster, use Mill's `assembly` task
// to create a fat JAR containing all dependencies:

/** See Also: spark-submit.sh */

// The `spark-submit.sh` script handles Spark installation (if needed) and
// runs the application. It accepts the JAR path, main class, and optional
// arguments:

/** Usage

> chmod +x spark-submit.sh

> ./mill show assembly
".../out/assembly.dest/out.jar"

> ./spark-submit.sh out/assembly.dest/out.jar foo.Foo resources/transactions.csv
...
Summary Statistics by Category:
+-----------+------------+--------------+-----------------+
|   category|total_amount|average_amount|transaction_count|
+-----------+------------+--------------+-----------------+
|       Food|        70.5|          23.5|                3|
|Electronics|       375.0|         187.5|                2|
|   Clothing|       120.5|         60.25|                2|
+-----------+------------+--------------+-----------------+
...
*/

// === Testing
//
// The test suite validates the aggregation logic using in-memory test data:

/** See Also: test/src/FooTests.scala */

// This pattern of testing Spark transformations with small in-memory datasets
// allows for fast, reliable unit tests without needing external data sources
// or cluster resources.
