// tag::header[]

# Selective Test Execution in Mill


:author: Li Haoyi
:revdate: ???
_{author}, {revdate}_

include::mill:ROOT:partial$gtag-config.adoc[]

Selective test execution is a key build tool feature necessary for working with any
large monorepo. This blog post will explore what exactly selective test execution is
all about, how Mill supports it, and what are the limitations.

// end::header[]


The nature of a monorepo is that it contains a large amount of code, of which you
are only working on a fraction of it at any point in time. For example, a large monorepo
may have subfolders containing "modules" or "subprojects" for:

1. 3 backend services: `backend_service_1`, `backend_service_2`, `backend_service_3`
2. 2 frontend web codebases: `frontend_web_1`, `frontend_web_2`
3. 1 `backend_service_utils` module, and 1 `frontend_web_utils` module
4. 3 deployments: `deployment_1`, `deployment_2`, `deployment_3`, that make use of the
   backend services and frontend web codebases.
5. The three deployments may be combined into a `staging_environment`, which is then
   used in three sets of `end_to_end_tests`, one for each deployment

These modules may depend on each other as shown in the diagram below,
with the various `backend_service` and `frontend_web` codebases
grouped into `deployments`, and the `_utils` files shared between them:

```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0 style=filled fillcolor=white]

  frontend_web_utils -> frontend_web_2 -> deployment_2
  frontend_web_utils -> frontend_web_1 -> deployment_1

  backend_service_utils -> backend_service_3 -> deployment_3
  backend_service_utils -> backend_service_2 -> deployment_2
  backend_service_utils -> backend_service_1 -> deployment_1
  deployment_1 -> staging_environment
  deployment_2 -> staging_environment
  deployment_3 -> staging_environment
  staging_environment -> end_to_end_tests_1
  staging_environment -> end_to_end_tests_2
  staging_environment -> end_to_end_tests_3
}
```

These various modules would typically each have their own test suite, e.g.

* Each backend
  service would have `backend_service_1_tests`, `backend_service_2_tests`,
* Each frontend
  web codebase would have  `frontend_web_1_tests`, `frontend_web_2_tests`,
* Utility modules would have
  `backend_service_utils_tests`, `frontend_web_utils_tests`, etc.
* The deployments
  will have integration or end-to-end tests to verify that everything is wired up correctly.
* The entire system is aggregated into a `staging_environment`, which is then used
  to run `end_to_end_tests`

## Naive Global Test Selection

Most software projects start off naively running every test on every pull request
to validate the changes.

For most small projects this is fine, and most projects start small and stay small so this
is not a problem. However, for the projects that do continue growing, this strategy quickly
becomes infeasible:

* The size of the codebase (in number of modules, lines of code, or number of tests) grows
  linearly `O(n)` over time
* The number of tests necessary to validate any pull request also grows linearly `O(n)`

This means that although the test runs may start off running quickly, they naturally slow
down over time: a week-old project may have a test suite that runs in seconds, by a few
months it may start taking minutes to run, and after a few years the tests may be taking
over an hour. And there is no upper bound: in a large growing project test runs can easily
take several hours or even days to run, becoming a real bottleneck in how fast developers
can make progress and merge changes into the project.

Naive global test execution works best for small projects with 1-10 developers, but beyond that
the inefficiency of running tests not relevant to your changes quickly becomes noticeable and
starts slowing things down


## Module-based Test Selection

Typically, in any large codebase, most work happens within a single part of it: e.g. a
developer may be working on `backend_service_1`, another may be working on `frontend_web_2`.
The obvious thing to do would be make sure each module is in its own folder, e.g.

- `backend_service_1/`
- `backend_service_2/`
- `backend_service_3/`
- `frontend_web_1/`
- `frontend_web_2/`
- `backend_service_utils/`
- `frontend_web_utils/`
- `deployment_1/`
- `deployment_2/`
- `deployment_3/`

To do simple module-based selective execution generally involves:

1. You need to know which modules own which source files (e.g. based on the folder),
2. Run a `git diff` on the code change you want to validate
3. For any modules which contain changed files, run their tests

For example,

- Someone making changes in `backend_service_1/` will need to run the `backend_service_1/` tests
- Someone making changes in `frontend_web_2/` will need to run the `frontend_web_2/` tests

This does limit the number of tests any individual needs to execute: someone working on
`backend_service_1` does not need to run the tests for `backend_service_2`, `backend_service_3`,
etc.. This helps keep the test times when working on the monorepo from growing unbounded.

However, module-based selective test execution is not enough: it is possible that changing
a module would not break its own tests, but it could cause problems with downstream modules that
depend on it:

1. Changing `backend_service_1` may require corresponding changes to `frontend_web_1`, and
   if those changes are not coordinated the integration tests in `deployment_1` would fail

2. Changing `frontend_web_utils` could potentially break both `frontend_web_1` and `frontend_web_2`
   that depend on it, and thus we need to run the tests for both those modules to validate our change.

Module-based test selection works fine for codebases with 10-100 developers: there are
occasional cases where a breakage might slip through, but generally it's infrequent enough
that it's tolerable. But as the development organization grows beyond 100, these breakages
affect more and more people and become more and more painful. To resolve this, we need
something more sophisticated.

## Dependency-based Test Selection

To solve the problem of code changes potentially breaking downstream modules, we need to make
sure that for every code change, we run both the tests for that module as well as every downstream
test. For example, if we make a change to `backend_service_1`, we need to run the unit tests for
`backend_service_1` as well as the integration tests for `deployment_1`:


```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0 style=filled fillcolor=white]

  frontend_web_utils -> frontend_web_2 -> deployment_2
  frontend_web_utils -> frontend_web_1 -> deployment_1

  backend_service_utils -> backend_service_3 -> deployment_3
  backend_service_utils -> backend_service_2 -> deployment_2

  backend_service_utils -> backend_service_1
  backend_service_1 [color=red, penwidth=2]
  deployment_1 [color=red, penwidth=2]
  backend_service_1 -> deployment_1 [color=red, penwidth=2]
}
```

On the other hand, if we make a change to `frontend_web_utils`, we need to run the unit tests
for `frontend_web_1` and `frontend_web_2`, as well as the integration tests for `deployment_1`
and `deployment_2`, but _not_ `deployment_3` since it doesn't depend on any frontend codebase:

```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0 style=filled fillcolor=white]
  frontend_web_utils [color=red, penwidth=2]
  frontend_web_2 [color=red, penwidth=2]
  deployment_2 [color=red, penwidth=2]
  frontend_web_1 [color=red, penwidth=2]
  deployment_1 [color=red, penwidth=2]

  frontend_web_utils -> frontend_web_1 -> deployment_1 [color=red, penwidth=2]
  frontend_web_utils -> frontend_web_2 -> deployment_2 [color=red, penwidth=2]

  backend_service_utils -> backend_service_3 -> deployment_3
  backend_service_utils -> backend_service_2 -> deployment_2
  backend_service_utils -> backend_service_1 -> deployment_1
}
```

This kind of dependency-based selective test execution is generally straightforward:

1. You need to know which modules own which source files (e.g. based on the folder),
2. You need to know which modules depend on which other modules
3. Run a `git diff` on the code change you want to validate
4. For any modules which contain changed files, run a breadth-first traversal of the module graph
5. For all the modules discovered during the traversal, run their tests

The algorithm (i.e. a breadth first search) is pretty trivial, the interesting part
is generally how you know _"which modules own which source files"_ and
_"which modules depend on which other modules"_. For smaller projects this can be
managed manually in a bash or python script, e.g.
https://github.com/apache/spark/blob/290b4b31bae2e02b648d2c5ef61183f337b18f8f/dev/sparktestsupport/modules.py#L108-L126[this code in Apache Spark]
that manually maintains a list of source folders and dependencies per-module.
In a larger project maintaining this information by hand is tedious and error prone,
so it is better to get the information from your build tool that already has it
(e.g. via xref:mill::large/selective-execution.adoc[Mill Selective Execution]).

Dependency-based selective test execution can get you pretty far: 100s to 1,000 developers
working on a shared codebase. But it still has weaknesses, and as the number of
developers grows beyond 1,000, you begin noticing issues and inefficiency:

1. You are limited by the granularity of your module graph. For example,
   `backend_service_utils` may be used by all three ``backend_service``s,
   but not _all_ of `backend_service_utils` is used by all three services. Thus
   a change to `backend_service_utils` may result in running tests for all three
   ``backend_service``s, even if that change may not affect that particular service

2. You may over-test things redundantly. For example, a function in `backend_service_utils`
   may be exhaustively tested, and running unit tests for all three ``backend_service``s
   as well as integration tests for all three ``deployment``s may be unnecessary, as they
   will just exercise code paths that are already exercised as part of the `backend_service_utils`
   test suite

3. These failure modes are especially problematic for integration or end-to-end tests.
   The nature of end-to-end tests is that they depend on _everything_, and so you find
   _any change_ in your codebase triggering _every end-to-end_test_ to be run. These
   are also the slowest tests in your codebase, so running every end-to-end test every time
   you touch any line of code is extremely expensive and wasteful.

For example, touching `backend_service_1` is enough to trigger all the `end_to_end` tests:

```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0 style=filled fillcolor=white]

  frontend_web_utils -> frontend_web_2 -> deployment_2
  frontend_web_utils -> frontend_web_1 -> deployment_1

  backend_service_utils -> backend_service_1
  backend_service_1 [color=red, penwidth=2]
  deployment_1 [color=red, penwidth=2]
  backend_service_1 -> deployment_1 [color=red, penwidth=2]
  backend_service_utils -> backend_service_2 -> deployment_2
  backend_service_utils -> backend_service_3 -> deployment_3

  deployment_1 -> staging_environment
  deployment_2 -> staging_environment
  deployment_3 -> staging_environment
  staging_environment [color=red, penwidth=2]
  end_to_end_tests_1 [color=red, penwidth=2]
  end_to_end_tests_2 [color=red, penwidth=2]
  end_to_end_tests_3 [color=red, penwidth=2]

  staging_environment -> end_to_end_tests_1 [color=red, penwidth=2]
  staging_environment -> end_to_end_tests_2 [color=red, penwidth=2]
  staging_environment -> end_to_end_tests_3 [color=red, penwidth=2]
}
```

Touching `frontend_web_2` is also enough to trigger all the `end_to_end` tests:

```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0 style=filled fillcolor=white]



  frontend_web_2 [color=red, penwidth=2]
  frontend_web_2 -> deployment_2  [color=red, penwidth=2]
  frontend_web_utils -> frontend_web_1 -> deployment_1
  frontend_web_utils -> frontend_web_2
  deployment_2 [color=red, penwidth=2]
  backend_service_utils -> backend_service_1
  backend_service_1 -> deployment_1
  backend_service_utils -> backend_service_2 -> deployment_2
  backend_service_utils -> backend_service_3 -> deployment_3

  deployment_1 -> staging_environment
  deployment_2 -> staging_environment
  deployment_3 -> staging_environment
  staging_environment [color=red, penwidth=2]
  end_to_end_tests_1 [color=red, penwidth=2]
  end_to_end_tests_2 [color=red, penwidth=2]
  end_to_end_tests_3 [color=red, penwidth=2]

  staging_environment -> end_to_end_tests_1 [color=red, penwidth=2]
  staging_environment -> end_to_end_tests_2 [color=red, penwidth=2]
  staging_environment -> end_to_end_tests_3 [color=red, penwidth=2]
}
```


While there are some ways you can improve the granularity of the module graph to improve
the precision of dependency-based selective test execution, generally that only gets you
small improvements, and everyone inevitable ends up using some kind of heuristics to trim
down the set of tests further.

## Heuristic-based Test Selection

The next stage of selective test execution is to use heuristics: these are ad-hoc rules
that you put in place to decide what tests to run based on a code change. Common heuristics
include:

1. Make dependency-based selective execution only traverse N layers of modules: the chances
   are that a breaking in module X will be caught by X's test suite, or the test suite of
   X's direct downstream modules, so we don't need to run every single transitive downstream
   module's test suite. e.g. if we set `N = 1`, then a change to `backend_service_1` will
   only run tests for `backend_service_1` and `deployment_1`, but not the `end_to_end_tests`
   downstream of the `staging_environment`:

```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0 style=filled fillcolor=white]

  frontend_web_utils -> frontend_web_2 -> deployment_2
  frontend_web_utils -> frontend_web_1 -> deployment_1

  backend_service_utils -> backend_service_1
  backend_service_1 [color=red, penwidth=2]
  deployment_1 [color=red, penwidth=2]
  backend_service_1 -> deployment_1 [color=red, penwidth=2]
  backend_service_utils -> backend_service_2 -> deployment_2
  backend_service_utils -> backend_service_3 -> deployment_3

  deployment_1 -> staging_environment
  deployment_2 -> staging_environment
  deployment_3 -> staging_environment
  staging_environment -> end_to_end_tests_1
  staging_environment -> end_to_end_tests_2
  staging_environment -> end_to_end_tests_3
}
```


2. Make end-to-end tests use module-based test selection for them instead of dependency-based
   test selection and use: e.g. if I change `deployment_1`, ignore the `staging_environment`
   when finding downstream tests, and only run `end_to_end_tests_1` since it is end-to-end
   tests for the `deployment_1` module:

```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0 style=filled fillcolor=white]

  frontend_web_utils -> frontend_web_2 -> deployment_2
  frontend_web_utils -> frontend_web_1 -> deployment_1

  backend_service_utils -> backend_service_3 -> deployment_3
  backend_service_utils -> backend_service_2 -> deployment_2
  backend_service_utils -> backend_service_1

  backend_service_1 [color=red, penwidth=2]
  deployment_1 [color=red, penwidth=2]
  backend_service_1 -> deployment_1 [color=red, penwidth=2]


  deployment_3 -> staging_environment -> end_to_end_tests_1 [style=dashed]

  deployment_2 -> staging_environment -> end_to_end_tests_2 [style=dashed]
  deployment_1 -> staging_environment -> end_to_end_tests_3 [style=dashed]
  staging_environment [style=dashed]


  end_to_end_tests_1 [color=red, penwidth=2]
  end_to_end_tests_2
  end_to_end_tests_3
}
```

3. Use machine learning to decide which tests to run, based on the contents of the `git diff`
   and the historical test runs.


The nature of heuristics is that they are _approximate_. That means that it is possible
both that we run too may tests and waste time, and also that we run too few tests and allow
a breakage to slip through. Typically this sort of heuristic is only used early in the testing
pipeline, e.g.

1. When validating pull requests, use heuristics to trim down the set of tests to run before merging
2. After merging, run a more thorough set of tests without heuristics
   to catch any bugs that slipped through and prevent bugs from being shipped to customers.
3. If a bug is noticed during post-merge testing, bisect it and revert/fix the offending commit

This may seem hacky and complicated, and bisecting/reverting commits post-merge can indeed
waste a lot of time. But such a workflow necessary in any large codebase and organization.
The heuristics also do not need to be 100% precise, and as long as they are precise _enough_ that
the time saved skipping tests outweighs the time spend dealing with post-merge breakages, it
still ends up being worth it.

## Selective Execution in Mill

Any project can use <<Naive Global Test Selection>> for free, and the Mill build tool's
xref:mill::large/selective-execution.adoc[Selective Test Execution] supports for
<<Dependency-based Test Selection>> out of the box. This makes it easy to set up
CI for your projects using Mill that only run tests that are downstream of the code
you changed in a pull request.