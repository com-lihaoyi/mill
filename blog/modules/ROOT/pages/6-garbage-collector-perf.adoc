// tag::header[]

# Understanding JVM Garbage Collector Performance


:author: Li Haoyi
:revdate: 3 January 2024
_{author}, {revdate}_

include::mill:ROOT:partial$gtag-config.adoc[]

Garbage collectors are a core part of many programming languages, but are surprisingly
poorly understood. This article
will discuss the fundamental design of how garbage collectors work, and tie it to real
benchmarks of how the JVM garbage collector performs running some synthetic workloads. You
should come away with a deeper understanding of how the JVM garbage collector works and
how you can work to optimize its performance in your own real-world projects.

// end::header[]

## A Theoretical Garbage Collector

To understand how the real-world JVM garbage collector works, its best to start from
a simplification: the simplest copying garbage collector. This will both give an intuition
for how things work in general, and also help you notice when things diverge from this
idealized example.

### Process Memory

At its core, a garbage collector helps manage the free memory of a program, often called the
_heap_. The memory of a program can be modelled as a linear sequence of storage locations, e.g.
below where we have 16 slots in memory:

```graphviz
digraph G {
  
  node [shape=box width=0 height=0 style=filled fillcolor=white]
  heap [shape=record label="HEAP | <f0> | <f1> | <f2> | <f3> | <f4> | <f5> | <f6> | <f7> | <f8> | <f9> | <f10> | <f11> | <f12> | <f13> | <f14> | <f15>"]

  heap:f0:s -> alloc:n [dir=back, style=dotted]
  alloc [label = "free memory", shape=plaintext]
}
```

These storage locations can contain objects (below named `foo`, `bar`, `qux`, `baz`) that take
up memory and may reference other objects (solid arrows). Furthermore, the values may be referenced from outside
the heap (dashed lines), e.g. from the "stack" which represents all objects referenced by
local variables in methods that are currently being run (shown below) or from static global
variables (not shown). We keep a `free memory` pointer to the first empty slot on the right.


```graphviz
digraph G {
  
  node [shape=box width=0 height=0 style=filled fillcolor=white]
  heap [shape=record label="HEAP | <f0> foo | <f1> bar | <f2> qux | <f3> baz | <f4> | <f5> | <f6> | <f7> | <f8> | <f9> | <f10> | <f11> | <f12> | <f13> | <f14> | <f15>"]
  heap:f0:s -> heap:f1:s
  heap:f0:s -> heap:f2:s
  heap:f2:n -> heap:f3:n
  heap:f4:s -> alloc:n [dir=back, style=dotted]
  alloc [label = "free memory", shape=plaintext]

  stack [shape=record label="STACK | <f0> | <f1> | <f2> | <f3> | <f4> | <f5> | <f6> | <f7> "]
  stack:f0 -> heap:f1 [dir=none, style=dashed]
  stack:f1 -> heap:f2 [dir=none, style=dashed]
}
```

If we want to allocate a new object `new1`, we can simply put it at the location of
the `free memory` pointer (green below), and bump `free memory` 1 slot to the right

```graphviz
digraph G {

  node [shape=box width=0 height=0 style=filled fillcolor=white]
  heap [shape=record label="HEAP | <f0> foo | <f1> bar | <f2> qux | <f3> baz | <f4> new1 | <f5> | <f6> | <f7> | <f8> | <f9> | <f10> | <f11> | <f12> | <f13> | <f14> | <f15>"]
  heap:f0:s -> heap:f1:s
  heap:f0:s -> heap:f2:s
  heap:f2:n -> heap:f3:n
  heap:f5:s -> alloc:n [dir=back, style=dotted]
  alloc [label = "free memory", shape=plaintext]

  stack [shape=record label="STACK | <f0> | <f1> | <f2> | <f3> | <f4> | <f5> | <f6> | <f7> "]
  stack:f0 -> heap:f1 [dir=none, style=dashed]
  stack:f1 -> heap:f2 [dir=none, style=dashed]
  stack:f2 -> heap:f4 [dir=none, style=dashed,  color=green, penwidth=3]
}
```

For the purposes of this example, we show all objects on the heap taking up 1 slot, but
in real programs the size of each object may vary depending on how many fields it has
or if it's a variable-length array.

### A Simple Garbage Collector

The simplest kind of garbage collector splits the 16-slot heap we saw earlier is split into
two 8-slot halves. If
we want to allocate 4 more objects (`new2`, to `new5`), but there are only 3 slots left on the
heap, we will need to do a garbage collection:

```graphviz
digraph G {
  
  node [shape=box width=0 height=0 style=filled fillcolor=white]
  {rank=same; heap1; heap2}
  heap2 [shape=record label="HALF2 | <f0> | <f1> | <f2> | <f3>  | <f4> | <f5> | <f6> | <f7> "]

  heap1 [shape=record label="HALF1 | <f0> foo | <f1> bar | <f2> baz | <f3> qux | <f4> new1 | <f5> | <f6> | <f7> "]
  heap1:f0:s -> heap1:f1:s
  heap1:f0:s -> heap1:f2:s
  heap1:f2:n -> heap1:f3:n
  heap1:f5:s -> alloc:n [dir=back, style=dotted]
  alloc [label = "free memory", shape=plaintext]

  stack [shape=record label="STACK | <f0> | <f1> | <f2> | <f3> | <f4> | <f5> | <f6> | <f7> "]
  stack:f0 -> heap1:f1 [dir=none, style=dashed]
  stack:f1 -> heap1:f2 [dir=none, style=dashed]
  stack:f2 -> heap1:f4 [dir=none, style=dashed]
}
```

To do a garbage collection, the simplest garbage collector first starts from all non-heap
references (e.g. the `STACK` references above) often called "heap roots". It then traces
the graph of references, highlighted red below:

```graphviz
digraph G {
  
  node [shape=box width=0 height=0 style=filled fillcolor=white]
  {rank=same; heap1; heap2}
  heap2 [shape=record label="HALF2 | <f0> | <f1> | <f2> | <f3>  | <f4> | <f5> | <f6> | <f7> "]

  heap1 [shape=record label="HALF1 | <f0> foo | <f1> bar | <f2> qux | <f3> baz | <f4> new1| <f5> | <f6> | <f7> "]
  heap1:f0:s -> heap1:f1:s
  heap1:f0:s -> heap1:f2:s
  heap1:f2:n -> heap1:f3:n [color=red, penwidth=3]
  heap1:f5:s -> alloc:n [dir=back, style=dotted]
  alloc [label = "free memory", shape=plaintext]
  stack [shape=record label="STACK | <f0> | <f1> | <f2> | <f3> | <f4> | <f5> | <f6> | <f7> "]
  stack:f0 -> heap1:f1 [dir=none, style=dashed, color=red, penwidth=3]
  stack:f1 -> heap1:f2 [dir=none, style=dashed, color=red, penwidth=3]
  stack:f1 -> heap1:f4 [dir=none, style=dashed, color=red, penwidth=3]
}
```

Here, we can see that `foo` is not referenced, `bar` and `qux` are referenced directly from the
`STACK`, and `baz` is referenced indirectly from `qux`.

We then copy all objects we traced (often called the _live-set_) from `HALF1` to `HALF2`, adjust all the references
appropriately. Now `HALF2` is the half of the heap in use, and `HALF1` can be reset to empty.
We now have space to allocate the 4 `new2` to `new5` objects we wanted (green):

```graphviz
digraph G {
  
  node [shape=box width=0 height=0 style=filled fillcolor=white]
  {rank=same; heap1; heap2}
  heap2 [shape=record label="HALF2 | <f0> bar | <f1> qux | <f2> baz | <f3> new1 | <f4> new2 | <f5> new3 | <f6> new4 | <f7> new5 "]

  heap1 [shape=record label="HALF1 | <f0> | <f1> | <f2> | <f3> | <f4> | <f5> | <f6> | <f7> "]
  heap2:f1:s -> heap2:f2:s [color=red, penwidth=3]

  stack [shape=record label="STACK | <f0> | <f1> | <f2> | <f3> | <f4> | <f5> | <f6> | <f7> "]
  stack:f0 -> heap2:f0 [dir=none, style=dashed, color=red, penwidth=3]
  stack:f1 -> heap2:f1 [dir=none, style=dashed, color=red, penwidth=3]
  stack:f2 -> heap2:f3 [dir=none, style=dashed, color=red, penwidth=3]
  stack:f3 -> heap2:f4 [dir=none, style=dashed, color=green, penwidth=3]
  stack:f4 -> heap2:f5 [dir=none, style=dashed, color=green, penwidth=3]
  stack:f5 -> heap2:f6 [dir=none, style=dashed, color=green, penwidth=3]
  stack:f6 -> heap2:f7 [dir=none, style=dashed, color=green, penwidth=3]

}
```

You may notice that the object `foo` has disappeared. This is because `foo` was not
referenced directly by any stack reference or indirectly: it was unreachable, and thus
considered "garbage". `foo` was
not explicitly deleted, but rather simply did not get copied over from `HALF1` to `HALF2`
during collection, and thus was wiped out when `HALF1` was cleared.

As your program executes, the methods actively running may change, and thus the references
(both from stack to heap and within entries on your heap) may change. For example, we may
stop referencing `bar`, `new2`, and `new3`:

```graphviz
digraph G {
  
  node [shape=box width=0 height=0 style=filled fillcolor=white]
  {rank=same; heap1; heap2}
  heap2 [shape=record label="HALF2 | <f0> bar | <f1> qux | <f2> baz | <f3> new1 | <f4> new2 | <f5> new3 | <f6> new4 | <f7> new5 "]

  heap1 [shape=record label="HALF1 | <f0> | <f1> | <f2> | <f3> | <f4> | <f5> | <f6> | <f7> "]
  heap2:f1:s -> heap2:f2:s

  stack [shape=record label="STACK | <f0> | <f1> | <f2> | <f3> | <f4> | <f5> | <f6> | <f7> "]

  stack:f1 -> heap2:f1 [dir=none, style=dashed]
  stack:f2 -> heap2:f3 [dir=none, style=dashed]


  stack:f5 -> heap2:f6 [dir=none, style=dashed]
  stack:f6 -> heap2:f7 [dir=none, style=dashed]


}
```

Although `bar`, `new2` and `new3` are now "garbage", our heap is still "full". Thus, if we want
to allocate a new object (e.g. `new6`) we need to repeat the garbage collection process: tracing
the objects transitively reachable (`qux`, `bax`, `new1`, `new4`, `new5`), copying them
from `HALF2` to `HALF1`,
adjusting any references to now use `HALF1` as the new heap, and clearing anything that was left
behind in `HALF2`.

```graphviz
digraph G {
  
  node [shape=box width=0 height=0 style=filled fillcolor=white]
  {rank=same; heap1; heap2}
  heap2 [shape=record label="HALF2 | <f0> | <f1>  | <f2>  | <f3>  | <f4> | <f5> | <f6>  | <f7>  "]

  heap1 [shape=record label="HALF1 | <f0> qux | <f1> baz | <f2> new1 | <f3> new4 | <f4> new5 | <f5> new6 | <f6> | <f7> "]
  heap1:f0:s -> heap1:f1:s [color=red, penwidth=3]

  stack [shape=record label="STACK | <f0> | <f1> | <f2> | <f3> | <f4> | <f5> | <f6> | <f7> "]

  stack:f1 -> heap1:f0 [dir=none, style=dashed, color=red, penwidth=3]
  stack:f2 -> heap1:f2 [dir=none, style=dashed, color=red, penwidth=3]


  stack:f5 -> heap1:f3 [dir=none, style=dashed, color=red, penwidth=3]
  stack:f6 -> heap1:f4 [dir=none, style=dashed, color=red, penwidth=3]
  stack:f7 -> heap1:f5 [dir=none, style=dashed, color=green, penwidth=3]
  heap1:f6:s -> alloc:n [dir=back, style=dotted]
  alloc [label = "free memory", shape=plaintext]
}
```

This process can repeat as many times as necessary: as long as there are _some_ objects
that are unreachable, you can run a GC cycle and copy the "live" objects to the other
half of the heap, freeing up some space to allocate new objects. The only reason this
may fail is that if you run a GC cycle and there _still_ isn't enough space to allocate
the objects you want; that means your program  has run out of memory, and will fail with
an `OutOfMemoryError` or similar.

Even this simple GC has a lot of interesting properties, and you may have heard these
terms or labels that can apply to it:

* *semi-space* garbage collector, because of the way it splits the heap into two halves

* *copying* garbage collector, because it needs to copy the heap objects back and forth

* *tracing* garbage collector, because of the way it traces the graph of heap
  references in order to decide what to copy.

* *stop the world* garbage collector, because while this whole trace-copy-update-references
  workflow is happening, we have to stop the program to avoid race conditions between the garbage
  collector and the program code.

* *compacting* garbage collector, because every time we run a GC, we copy everything to the
  left-most memory, avoiding the memory fragmentation that occurs with other memory
  management techniques such as https://en.wikipedia.org/wiki/Reference_counting[Reference Counting].

Most modern GCs are considerably more complicated than this: e.g. they may have optimizations
to avoid wasting half the heap by leaving it empty, or they may have
xref:_generational_optimizations[optimizations for handling short-lived objects], but at
their heart this is still what they do. And understanding the performance characteristics of
this simple, naive GC can help give you an intuition in how modern high-performance GCs work.


### Theoretical GC Performance

Typically, GC performance focuses on two main aspects:

- *Throughput*: what % of the time your program is spending on real work, rather than collecting garbage
- *Pause Times*: what is the longest time your program is spent stuck collecting garbage and making zero progress on real work

These two metrics are separate:

* *Some programs only care about throughput*, e.g. if you only care about how long a big batch
  analysis takes to complete, and don't care if it pauses in the middle to GC: you just want
  it to finish as soon as possible
* *Other programs only care about pause times*, e.g. someone playing a videogame doesn't care if
  it can run faster than their eye can perceive, but they do care that it does not freeze or
  pause for noticeable amounts of time when it needs to run a GC cycle

Even from the limited description above, we can already make some interesting inferences
about how the performance of a simply garbage collector will be like.

1. *Allocations in garbage collectors are _cheap_*: when the heap is not yet full, we can
   just allocate things on the first empty slots on the right side of the heap and bump `free-pointer`,
   without having to scan the heap to find empty slots.

2. *Pause times should be proportional to the size of the live-set*. That is because
   A GC cycle involves tracing, copying, then updating the references within the live-set.

3. *Pause times would _not_ depend on the amount of garbage to be collected*. The GC cycle
   we looked at above spend no time at all looking at or scanning for garbage objects,
   they simply all disappeared when their half of the heap was wiped out following a collection.

4. *Interval between GC cycles is inversely proportional to free memory*.
   We only need to run a GC cycle when the garbage we allocate fills up the "extra" heap memory
   our program has on top of what is necessary to store the live-set.

5. *GC throughput is the pause time divided by the interval, or proportional
   to the extra memory and inversely proportional to the live-set size and heap size*

In other words:

* `allocation_cost = O(1)`

* `gc_pause_time = O(live-set)`

* `gc_interval = O((heap-size - live-set) / throughput)`

* `gc_throughput = gc_interval / gc_pause_time = O((heap-size - live-set) / throughput / live-set)`

Even from this small conclusions, we can already see some unintuitive results:

1. *More memory does _not_ reduce pause times!* `gc_pause_time = O(live-set)`, and so
   pause times do not depend on how much `heap-size` you have.

2. *There is no point at which providing more memory does not improve GC throughput!*
   `gc_throughput = O((heap-size - live-set) / throughput / live-set)`, so
   providing larger and larger ``heap-size``s means GCs happen less and less frequently, meaning a
   larger % of your program time is spent on useful work. But there is no "maximum" and
   you can always add more memory to eke out a bit more performance.

3. *Conversely, providing exactly as much memory as the program requires_ is the worst
   case possible!* `gc_throughput = O((heap-size - live-set) / throughput / live-set)` when
   `heap-size = live-set` means `gc_interval = 0` and `gc_throughput = 0`: every single allocation the program will
   need to run an expensive GC cycle to free up very little memory, and your program will
   spend all its time running GC cycles. Garbage collectors therefore _need_
   excess memory to work with, on top of the memory you would expect to need to allocate
   all the objects in your program.

Even from this theoretical analysis, we have already found a number of surprising results
in how GCs perform over time. Let's now see how this applies to some real-world garbage
collectors included with the Java Virtual Machine

## Benchmarking JVM Garbage Collectors

Now that we have run through a theoretical introduction and analysis of how GCs work
and how we would expect them to perform, let's look at some small Java programs and
monitor how garbage collection happens when using them:

```java
public class GC {
    public static void main(String[] args) throws Exception{
        final long liveSetByteSize = Integer.parseInt(args[0]) * 1000000L;
        final int benchMillis = Integer.parseInt(args[1]);
        final int benchCount = Integer.parseInt(args[2]);
        // 0-490 array entries per object, * 4-bytes per entry,
        // + 20 byte array header = average 1000 bytes per entry
        final int maxObjectSize = 490;
        final int averageObjectSize = (maxObjectSize / 2) * 4 + 20;

        final int liveSetSize = (int)(liveSetByteSize / averageObjectSize);

        long maxPauseTotal = 0;
        long throughputTotal = 0;

        for(int i = 0; i < benchCount + 1; i++) {
            int chunkSize = 256;
            Object[] liveSet = new Object[liveSetSize];
            for(int j = 0; j < liveSetSize; j++) liveSet[j] = new int[j % maxObjectSize];
            System.gc();
            long maxPause = 0;
            long startTime = System.currentTimeMillis();

            long loopCount = 0;
            java.util.Random random = new java.util.Random(1337);
            int liveSetIndex = 0;

            while (startTime + benchMillis > System.currentTimeMillis()) {
                if (loopCount % liveSetSize == 0) Thread.sleep(1);
                long loopStartTime = System.currentTimeMillis();
                liveSetIndex = random.nextInt(liveSetSize);
                liveSet[liveSetIndex] = new int[liveSetIndex % maxObjectSize];
                long loopTime = System.currentTimeMillis() - loopStartTime;
                if (loopTime > maxPause) maxPause = loopTime;
                loopCount++;
            }
            if (i != 0) {
                long benchEndTime = System.currentTimeMillis();
                long bytesPerLoop = maxObjectSize / 2 * 4 + 20;
                throughputTotal += (long) (1.0 * loopCount * bytesPerLoop / 1000000 / (benchEndTime - startTime) * averageObjectSize);
                maxPauseTotal += maxPause;
            }

            System.out.println(liveSet[random.nextInt(liveSet.length)]);
        }

        long maxPause = maxPauseTotal / benchCount;
        long throughput = throughputTotal / benchCount;

        System.out.println("longest-gc: " + maxPause + "ms, throughput: " + throughput + " mb/s");
    }
}
```

This is a small Java program designed to do a rough benchmark of Java garbage collection
performance. For each benchmark, it:

1. Starts off allocating a bunch of `int[]` arrays of varying size in `liveSet`, on
   average taking up 1000 bytes each.

2. Loops continuously to allocate more ``int[]``s and over-writes the references
   to older ones

3. Tracks how long each allocation takes to run: ideally it should be almost instant, but if
   that allocation triggers a GC it may take some time

4. Lastly, we print out the two numbers we care about
   in a GC: the `maxPause` time in milliseconds, and the `throughput` it is able to handle
   in megabytes per second.

This is a bit of a synthetic benchmark, and much less complex than a real-world
application would be, but it is enough for the purposes of this article.

You can run this program via:

```bash
> javac GC.java
> java -Xmx1g GC 800 10000 5 # Default is -XX:+UseG1GC
> java -Xmx1g -XX:+UseParallelGC GC 800 10000 5
> java -Xmx1g -XX:+UseZGC GC 800 10000 5
```

Above, `-Xmx1g` sets the heap size, the `-XX:` flags set the garbage collector, 800 sets
the `liveSet` size (in megabytes), and `10000` and `5` set the duration and number of
iterations to run the benchmark (here 10 seconds, 5 iterations). The measured pause times
and allocation rate are averaged over those 5 iterations.

### G1 Garbage Collector Benchmarks

Running this on the default GC (G1), we get the followings numbers:

*Pause Times*
[%autowidth.stretch, cols=">,>,>,>,>,>"]
|===
| live-set\heap-size | 800 mb | 1600 mb | 3200 mb | 6400 mb | 12800 mb
| 400 mb | 66 ms | 60 ms | 78 ms | 93 ms | 135 ms
| 800 mb |  | 108 ms | 93 ms | 160 ms | 262 ms
| 1600 mb |  |  | 158 ms | 164 ms | 564 ms
| 3200 mb |  |  |  | 331 ms | 5909 ms
| 6400 mb |  |  |  |  | 14222 ms
|===

*Throughput*
[%autowidth.stretch, cols=">,>,>,>,>,>"]
|===
| live-set\heap-size | 800 mb | 1600 mb | 3200 mb | 6400 mb | 12800 mb
| 400 mb | 3140 mb/s | 3823 mb/s | 5087 mb/s | 4871 mb/s | 5199 mb/s
| 800 mb |  | 3022 mb/s | 3598 mb/s | 4374 mb/s | 3741 mb/s
| 1600 mb |  |  | 2911 mb/s | 3440 mb/s | 2095 mb/s
| 3200 mb |  |  |  | 2797 mb/s | 888 mb/s
| 6400 mb |  |  |  |  | 338 mb/s
|===

As mentioned earlier, garbage collectors require some amount of free space in order to
work well, and so we only ran the benchmarks where the `heap-size` was twice or more
of the `live-set` size.

Above, we can see the behavior we discussed earlier:

1. GC pause times go up as the size of the live set increases. With a `800 mb` heap and
   `400 mb` live set the average pause time is `66 ms`, and it scales smoothly up to a
   `6400 mb` heap and `3200 mb` live set where the pause time is `331 ms`

2. GC pause times are relatively constant regardless of the heap size:
   e.g. for `400 mb` live set a `800 mb` heap has a `66 ms` pause time, while a `400 mb` live set and
  `6400 mb` heap (8 times as large!) has a `93 ms` pause time.

3. GC throughput goes up as the heap size increases, for a `400 mb` live set it goes
   smoothly from `3140 mb/s` for a `800 mb` heap to a `4871 mb/s` pause time for a `6400 mb`
   heap.

4. Both pause times and throughput seems to suffer for very large live sets and very large
   heaps: the `12800 mb` heap has disproportionately higher pause times compared to smaller
   heap sizes, even compared to `6400 mb`, regardless of live-set size.

### Generational Optimizations

One additional GC behavior worth discussing is the "Generational Hypothesis".
The idea is that _"most"_ objects do not live a long time, e.g. objects allocated within a method are often
collect when the method returns. Given that assumption, many GCs have made optimizations
for the collection of objects that become garbage quickly, such that collecting them is
much cheaper. Practically, that means that the same live-set and allocation rate can have
vastly different performance depending on how the allocations are structured:

1. "Least Recently Used" garbage collections, where the _oldest_ objects are the ones that
   get collected, will perform the worst

2. "Most Recently Used" garbage collections, where the _newest_ objects are the ones that
   get collected, will perform the best

Notable, "LRU" is one of the most common caching strategies, which it is possible
for in-memory caches with LRU cache eviction to make GC problems worse!

The example Java benchmark above keeps objects around a while before they become garbage,
by assigning new allocations to randomly indices in the `liveSet` array.
We can instead always assign new allocations to indices via a
https://en.wikipedia.org/wiki/Random_walk[Random Walk]:
randomly adjacent to the left or right of the previously-assigned allocation, meaning
that recently allocated objects are likely to be over-written (becoming unreachable and
eligible for garbage collection) more quickly by newer allocations
in the same part of `liveSet`, while older objects in other parts of `liveSet` are less
likely to be become unreachable. This lets us emulate the "generational" behavior that
is common in real-world programs:

```diff
-liveSetIndex = random.nextInt(liveSetSize);
+liveSetIndex += (random.nextBoolean() ? 1 : -1) + liveSetSize;
+liveSetIndex %= liveSetSize;
```


If we do this and measure the pause times and throughput of the example program,
we get the following:

*Pause Times*
[%autowidth.stretch, cols=">,>,>,>,>,>"]
|===
| live-set\heap-size | 800 mb | 1600 mb | 3200 mb | 6400 mb | 12800 mb
| 400 mb | 4 ms | 3 ms | 2 ms | 4 ms | 2 ms
| 800 mb |  | 3 ms | 3 ms | 12 ms | 3 ms
| 1600 mb |  |  | 5 ms | 10 ms | 4 ms
| 3200 mb |  |  |  | 13 ms | 11 ms
| 6400 mb |  |  |  |  | 22 ms
|===

*Throughput*
[%autowidth.stretch, cols=">,>,>,>,>,>"]
|===
| live-set\heap-size | 800 mb | 1600 mb | 3200 mb | 6400 mb | 12800 mb
| 400 mb | 7218 mb/s | 7495 mb/s | 7536 mb/s | 7550 mb/s | 7634 mb/s
| 800 mb |  | 7497 mb/s | 7790 mb/s | 7580 mb/s | 7819 mb/s
| 1600 mb |  |  | 7563 mb/s | 7464 mb/s | 7830 mb/s
| 3200 mb |  |  |  | 7128 mb/s | 5854 mb/s
| 6400 mb |  |  |  |  | 3286 mb/s
|===


Where the previous random-allocation benchmark has pause times of 10s to 100s to 1000s of
milliseconds, this "generational" benchmark has pause times in the 1s to 10s. The program
throughput is also significantly higher.
This demonstrates that the default G1 garbage collector does in fact have optimizations that
make it perform better for "generational" workloads.

Most GCs have some kind of optimization to make collecting recently-allocated objects
cheaper than collecting long-lived objects; these are often called _generational_
garbage collectors. Java's G1GC is no different, and we can see that even with
the same live-set size and heap sizes, shorter-lived objects are dramatically
cheaper to collect than long-lived objects.


### Z Garbage Collector Benchmarks

One interesting development in JVM garbage collectors is the
https://docs.oracle.com/en/java/javase/21/gctuning/z-garbage-collector.html[Z Garbage Collector].
This is a garbage collector that is optimized for lower pause times, exchange for requiring
much more memory than the default G1GC. If we run the benchmarks above with ZGC,
even without the <<Generational Optimizations>>, we get the numbers below:

*Pause Times*
[%autowidth.stretch, cols=">,>,>,>,>,>"]
|===
| live-set\heap-size | 800 mb | 1600 mb | 3200 mb | 6400 mb | 12800 mb
| 400 mb | 49 ms | 2 ms | 2 ms | 7 ms | 12 ms
| 800 mb |  | 71 ms | 1 ms | 1 ms | 5 ms
| 1600 mb |  |  | 136 ms | 1 ms | 5 ms
| 3200 mb |  |  |  | 346 ms | 5 ms
| 6400 mb |  |  |  |  | 886 ms
|===

*Throughput*
[%autowidth.stretch, cols=">,>,>,>,>,>"]
|===
| live-set\heap-size | 800 mb | 1600 mb | 3200 mb | 6400 mb | 12800 mb
| 400 mb | 2305 mb/s | 3959 mb/s | 4858 mb/s | 5318 mb/s | 3261 mb/s
| 800 mb |  | 2573 mb/s | 3888 mb/s | 4707 mb/s | 4645 mb/s
| 1600 mb |  |  | 2633 mb/s | 3555 mb/s | 3350 mb/s
| 3200 mb |  |  |  | 2381 mb/s | 2089 mb/s
| 6400 mb |  |  |  |  | 936 mb/s
|===

Some things worth noting with ZGC:

1. In the lower `heap-size` benchmarks - with `heap-size` twice `live-set` - ZGC
has similar pause times as the default G1GC (10s to 100s of milliseconds) but
with significantly worse throughput

2. For larger ``heap-size``s - 4 times the `live-set` and above - ZGC's pause times drop to
single-digit milliseconds (1-2 ms), much lower than those of G1GC

3. Z1 does seem to have slower pause times with large 12800 mb heaps as well, although
instead of 100s to 1000s of millisecond pause times it slows down to 5-10 ms.

As mentioned in the discussion on <<Theoretical GC Performance>>, for
most garbage collectors pause times are proportional to the live set, and increasing the
heap size does not help at all (and according to our <<G1 Garbage Collector Benchmarks>>, may
even make things worse!). This can be problematic, because there are many use cases that
cannot tolerate long GC pause times, but at the same time may require a significant amount
of live data to be kept in memory, so shrinking the live-set is not possible.

ZGC provides an option here, where if you are willing to provide _significantly_
more memory than the default G1GC requires, perhaps twice as much, you can get your pause times from
10-100s of milliseconds down to 1-2 milliseconds. And pause times remain low for a wide
range of heap sizes and live set sizes.


## GC Performance Takeaways


Now that we've studied garbage collections in theory, and looked at some concrete
numbers, there are some interesting conclusions that can be drawn:

1. *Caching data _in-process_ makes garbage collection pause times _worse_!* If your
   program in CPU-bottlenecked then caching to save computation can be worthwhile,
   but if you have problems with GC pause times then caching things in-memory will
   increase the size of your _live-set_ and therefore make your pause times even worse!
   "LRU" caches in particular are the worst case for garbage collectors, which are typically
   optimized for collecting recently-allocated short-lived objects.

2. *In contrast, caching things _out of process_ does not have this problem.* So feel free
   to cache things on disk, cache things in https://www.sqlite.org/[SQLite],
   https://github.com/redis/redis[Redis], https://memcached.org/[Memcached], and so on.
   These are not traced by your program's GC, and will not slow it in.

3. *Collecting Garbage doesn't actually slow the garbage collector down!* Most of the GC time
   is spent _tracing_ and _copying_ and live-set, and the garbage is basically ignored
   then wiped out in bulk. So it doesn't not matter how much garbage the GC has to collect:
   the time taken to run the GC cycle will be the same regardless

4. *There will never be an _exact_ amount of memory that a garbage-collected application
   needs.* You can _always_ increase throughput by providing more memory, to make GCs less
   and less frequent, leaving more time to do useful work. And you can usually provide
   less memory, at the cost of more and more frequent GCs. Exactly how much memory to
   provide is thus something you tweak and tune rather than something you can calculate exactly.

5. *Exactly when to declare the memory "enough" can be very arbitrary.*
   For example Java may throw an `OutOfMemoryError`
   https://docs.oracle.com/javase/8/docs/technotes/guides/troubleshoot/memleaks002.html[when
   it spends 98% of the time in Garbage Collection], which is an arbitrary threshold
   that is arguably too low (the program can still make progress after all!)
   and also arguably too high (do you really want to wait until your program slows down 50x
   before declaring there's not enough memory?)

6. *Shorter-lived objects are cheaper to collect*, due to most GCs being _generational_. This
   also ties into (1) above: caches tend to keep lots of long-lived objects in memory, which
   apart from slowing down GC cycles due to the size of the live-set, _also_ slows them down
   by missing out on the GC's optimizations for short-lived objects.

7. *Fewer larger processes can have worse GC performance than more smaller processes, even if
   they do the same thingds!* There are
   many ways in which consolidating smaller processes into larger ones can
   improve efficiency: less per-process overhead, eliminating
   https://en.wikipedia.org/wiki/Inter-process_communication[inter-process communication] cost,
   etc. But GC pause times scale with _total live set size_, so combining two smaller
   processes into one large one can make pause times _worse_ than they were before.
   And as we saw earlier, even within a single process allocating a large heap can make pause
   times worse


The Java benchmarks above were run on one particular set of hardware on one version
of the JVM, and the exact numbers will differ when run on other hardware or JVM versions.
Nevertheless, the overall trends that you can see would remain the same, as would the
take-aways of what you need to know to understand garbage collector performance.


## Conclusion

Garbage collectors can be complicated, differing in design
and implementation between languages (Python, Java, Go, etc.) and even within the same
language (Java's https://docs.oracle.com/en/java/javase/11/gctuning/parallel-collector1.html[ParallelGC],
https://docs.oracle.com/en/java/javase/17/gctuning/garbage-first-g1-garbage-collector1.html[G1GC],
the newer https://docs.oracle.com/en/java/javase/21/gctuning/z-garbage-collector.html[ZGC], etc.).
There are endless clever optimizations for the language designers to implement and knobs
for language users to tweak and tune.

However, at a high level most GCs are actually surprisingly similar, have the same
odd performance characteristics, and the same surprising pitfalls.
Hopefully this article will have given you a good intuition for how garbage collectors work and behave, so
next time you need to do something with your GC you have a solid understanding to work with.