<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <title>How To Manage Flaky Tests in your CI Workflows :: The Mill Build Tool</title>
    <link rel="canonical" href="https://mill-build.org/blog/4-flaky-tests.html" />
    <link rel="prev" href="5-executable-jars.html" />
    <link rel="next" href="3-selective-testing.html" />
    <meta name="generator" content="Antora 3.1.9" />
    <link rel="stylesheet" href="../_/css/site.css" />
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-1C582ZJR85"></script>
    <script>function gtag(){dataLayer.push(arguments)};window.dataLayer=window.dataLayer||[];gtag('js',new Date());gtag('config','G-1C582ZJR85')</script>
<link rel="icon" href="../_/favicon.png" type="image/x-icon" />
  
  <style>
  /* auto-hyphenation is super ugly */
  *{
    hyphens: manual !important;
  }

  /* Reduce font size from 17px to something reasonable */
  .doc {
   font-size: 16px !important;
  }
  .doc pre {
   font-size: 14px !important;
  }
  /* Shrink unreasonably large top bar */
  nav.navbar{
    height: 2.5rem;
  }
  body.article{
    padding-top: 2.5rem;
  }
  div.nav-container{
    top: 2.5rem;
  }
  div.toolbar{
    top: 2.5rem;
  }
  aside.nav{
    top: 2.5rem;
    height: calc(100vh - 2.5rem);
  }
  </style></head><body class="article">
  
<header class="header">
  
  <script>
  gtag('config', 'AW-16649289906');

  document.addEventListener('DOMContentLoaded', function() {
  const links = document.querySelectorAll('a');
    links.forEach(link => {
      if (link.textContent.trim().toLowerCase() === 'download') {
        link.addEventListener('click', function(event) {
          console.log("download link clicked")
          gtag('event', 'conversion', {'send_to': 'AW-16649289906/rsphCKfVq8QZELKBgIM-'});
        });
      }
    });
  });
  </script>
  <nav class="navbar">
    <div class="navbar-brand">
      <!-- TODO: add mill icon -->
      <a class="navbar-item" href="https://mill-build.org"><img src="../_/logo-white.svg" height="20" />Â The Mill Build Tool</a>
      <div class="navbar-item search">
        <input id="search-input" type="text" placeholder="Search the docs" />
      </div>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/com-lihaoyi/mill">GitHub</a>
        <a class="navbar-item" href="https://mill-build.org/blog">Blog</a>
        <a class="navbar-item" href="https://mill-build.org/api/latest/index.html">API</a>
        <a class="navbar-item" href="https://github.com/com-lihaoyi/mill/issues">Issues</a>
        <a class="navbar-item" href="https://github.com/com-lihaoyi/mill/discussions">Discuss</a>


            <!--
        <a class="navbar-item" href="#">Home</a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Documentation</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#"></a>
            <a class="navbar-item" href="#">Product B</a>
            <a class="navbar-item" href="#">Product C</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Plugins</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Service A</a>
            <a class="navbar-item" href="#">Service B</a>
            <a class="navbar-item" href="#">Service C</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Resources</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Resource A</a>
            <a class="navbar-item" href="#">Resource B</a>
            <a class="navbar-item" href="#">Resource C</a>
          </div>
        </div>
        <div class="navbar-item">
          <span class="control">
            <a class="button is-primary" href="#">Download</a>
          </span>
        </div>
        -->
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="blog" data-version="">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
<ul class="nav-list">

  <li class="nav-item" data-depth="0">
<ul class="nav-list">

  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="10-bytecode-analysis.html">Invalidating build caches using JVM bytecode callgraph analysis</a>
  </li>

  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="9-mill-faster-assembly-jars.html">Fast Incremental JVM Assembly Jar Creation with Mill</a>
  </li>

  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="8-what-is-a-build-tool.html">What does a Build Tool do?</a>
  </li>

  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="7-graal-native-executables.html">How to Compile Java into Native Binaries with Mill and Graal</a>
  </li>

  <li class="nav-item is-active" data-depth="1">
    <a class="nav-link" href="6-garbage-collector-perf.html">Understanding JVM Garbage Collector Performance</a>
  </li>

  <li class="nav-item is-active" data-depth="1">
    <a class="nav-link" href="5-executable-jars.html">How JVM Executable Assembly Jars Work</a>
  </li>

  <li class="nav-item is-current-page is-active" data-depth="1">
    <a class="nav-link" href="4-flaky-tests.html">How To Manage Flaky Tests in your CI Workflows</a>
  </li>

  <li class="nav-item is-active" data-depth="1">
    <a class="nav-link" href="3-selective-testing.html">Faster CI with Selective Testing</a>
  </li>

  <li class="nav-item is-active" data-depth="1">
    <a class="nav-link" href="2-monorepo-build-tool.html">Why Use a Monorepo Build Tool?</a>
  </li>

  <li class="nav-item is-active" data-depth="1">
    <a class="nav-link" href="1-java-compile.html">How Fast Does Java Compile?</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">The Mill Build Engineering Blog</span>
    <span class="version"></span>
  </div>
  <ul class="components">
    <li class="component">
      <div class="title"><a href="../mill/index.html">Mill Documentation</a></div>
      <ul class="versions">
        <li class="version">
          <a href="../mill/main-branch/index.html">main-branch</a>
        </li>
        <li class="version is-latest">
          <a href="../mill/index.html">0.12.8</a>
        </li>
        <li class="version">
          <a href="../mill/0.11.x/index.html">0.11.13</a>
        </li>
        <li class="version">
          <a href="../mill/0.10.15/index.html">0.10.15</a>
        </li>
        <li class="version">
          <a href="../mill/0.9.12/index.html">0.9.12</a>
        </li>
      </ul>
    </li>
    <li class="component is-current">
      <div class="title"><a href="index.html">The Mill Build Engineering Blog</a></div>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">

<div class="toolbar" role="navigation" style="position: fixed; top: 2.5rem; height: 2rem; left: 0px;">
<button class="nav-toggle"></button>
</div>
<div class="toolbar" style="position: fixed; top: 2.5rem; height: 1.5rem; right: 0px;">
<div class="edit-this-page"><a href="https://github.com/com-lihaoyi/mill/edit/main/out/website/blogFolder.dest/modules/ROOT/pages/4-flaky-tests.adoc">Edit this Page</a></div>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">How To Manage Flaky Tests in your CI Workflows</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p><em>Li Haoyi, 1 January 2025</em></p>
</div>
<div class="paragraph">
<p>Many projects suffer from the problem of flaky tests: tests that pass or fail
non-deterministically. These cause confusion, slow development cycles, and endless
arguments between individuals and teams in an organization.</p>
</div>
<div class="paragraph">
<p>This article dives deep into working with flaky tests, from the perspective of someone
who built the first flaky test management systems at Dropbox and Databricks and maintained
the related build and CI workflows over the past decade. The issue of flaky tests can be
surprisingly unintuitive, with many &quot;obvious&quot; approaches being ineffective
or counterproductive. But it turns out there <em>are</em> right and wrong answers to many of
these issues, and we will discuss both so you can better understand what managing flaky tests
is all about.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_what_causes_flaky_tests"><a class="anchor" href="#_what_causes_flaky_tests"></a>What Causes Flaky Tests?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A flaky test is a test that sometimes passes and sometimes fails, non-deterministically.</p>
</div>
<div class="paragraph">
<p>Flaky tests can be caused by a whole range of issues:</p>
</div>
<div class="sect2">
<h3 id="_race_conditions_within_tests"><a class="anchor" href="#_race_conditions_within_tests"></a>Race conditions within tests</h3>
<div class="paragraph">
<p>Often this manifest as <code>sleep</code>/<code>time.sleep</code>/<code>Thread.sleep</code> calls in your test
expecting some concurrent code path to complete, which may or may not wait
long enough depending on how much CPU contention there is slowing down your test code.
But any multi-threaded or multi-process code has the potential for race conditions or
concurrency bugs, and these days most systems make use of multiple cores.</p>
</div>
</div>
<div class="sect2">
<h3 id="_race_conditions_between_tests"><a class="anchor" href="#_race_conditions_between_tests"></a>Race conditions between tests</h3>
<div class="paragraph">
<p>Two tests both reading and writing
to the same global variables or files (e.g. in <code>~/.cache</code>) causing non-deterministic
outcomes. These can be tricky, because every test may pass when run alone, and the
test that fails when run in parallel may not be the one that is misbehaving!</p>
</div>
</div>
<div class="sect2">
<h3 id="_test_ordering_dependencies"><a class="anchor" href="#_test_ordering_dependencies"></a>Test ordering dependencies</h3>
<div class="paragraph">
<p>Even in the absence of parallelism, tests may interfere
with one another by mutating global variables or files on disk. Depending on the exact
tests you run or the order in which you run them, the tests can behave differently
and pass or fail unpredictably. Perhaps not strictly non-deterministic - the same
tests run in the same order will behave the same - but practically non-deterministic
since different CI runs may run tests in different orders.
<a href="3-selective-testing.html" class="xref page">Selective Testing</a> may cause this kind of issue,
or dynamic load-balancing of tests between parallel workers to minimize total wall
clock time (which <a href="https://github.com/dropbox/changes">Dropboxâs Changes CI system</a> did)</p>
</div>
</div>
<div class="sect2">
<h3 id="_resource_contention"><a class="anchor" href="#_resource_contention"></a>Resource contention</h3>
<div class="paragraph">
<p>Depending on exactly how your tests are run, they may rely
on process memory, disk space, file descriptors, ports, or other limited
physical resources. These are subject to noisy neighbour problems, e.g. an
overloaded linux system with too many processes using memory will start OOM-Killing
processes at random.</p>
</div>
</div>
<div class="sect2">
<h3 id="_external_flakiness"><a class="anchor" href="#_external_flakiness"></a>External flakiness</h3>
<div class="paragraph">
<p>Integration or end-to-end tests often
interact with third-party services, and it is not uncommon for the service
to be flaky, rate limit you, have transient networking errors, or just be entirely
down for some period of time. Even fundamental workflows like &quot;downloading
packages from a package repository&quot; can be subject to flaky failures</p>
</div>
<div class="paragraph">
<p>Sometimes the flakiness is test-specific, sometimes it is actually flakiness in the
code being tested, which may manifest as real flakiness when customers are trying
to use your software. Both scenarios manifest the same to developers - a test passing
and failing non-deterministically when run locally or in CI.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_why_are_flaky_tests_problematic"><a class="anchor" href="#_why_are_flaky_tests_problematic"></a>Why Are Flaky Tests Problematic?</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_development_slowdown"><a class="anchor" href="#_development_slowdown"></a>Development Slowdown</h3>
<div class="paragraph">
<p>Flaky tests generally make it impossible to know the state of your test suite,
which in turns makes it impossible to know whether the software you are working on
is broken or not, which is the reason you wanted tests in the first place.
Even a small number of flaky tests is enough to destroy the core value of your test suite.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Ideally, if the tests pass on a proposed code change, even someone
unfamiliar with the codebase can be confident that the code change did not break
anything.</p>
</li>
<li>
<p>Once test failures start happening spuriously, it quickly becomes
impossible to get a fully &quot;green&quot; test run without failures</p>
</li>
<li>
<p>So in order to make
validate a code change (merging a pull-request, deploying a service, etc.) the developer then
needs to individually triage and make judgements on those test failures to determine
if they are real issues or spurious</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This means that you are back to the &quot;manual&quot; workflow of developers squinting at
test failures to decide if they are relevant or not. This risks both false positives
and false negatives:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A developer may waste time triaging a failure that turns out to be a flake, and
unrelated to the change being tested</p>
</li>
<li>
<p>A developer may wrongly judge that a test failure is spurious, only for it to be a
real failure that causes breakage for customers once released.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Even relatively low rates of flakiness can cause issues. For example, consider the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A codebase with 10,000 tests</p>
</li>
<li>
<p>1% of the tests are flaky</p>
</li>
<li>
<p>Each flaky test case fails spuriously 1% of the time</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Although 1% of tests each failing 1% of the time may not seem like a huge deal, it means
that someone running the entire test suite only has a <code>0.99^100 = ~37%</code> chance of getting
a green test report! The other 63% of the time, someone running the test suite without any
real breakages gets one or more spurious failures, that they then have to spend time and energy
triaging and investigating. If the developer needs to retry the test runs to get a successful
result, they would need to retry on average <code>1 / 0.37 = 2.7</code> times: in this scenario
the retries alone may be enough to increase your testing latencies and infrastructure costs by
170%, on top of the manual work needed to triage and investigate the test failures!</p>
</div>
</div>
<div class="sect2">
<h3 id="_inter_team_conflict"><a class="anchor" href="#_inter_team_conflict"></a>Inter-team Conflict</h3>
<div class="paragraph">
<p>One fundamental issue with flaky tests is organizational:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The team that owns a test benefits from the test running and providing coverage
for the code they care about</p>
</li>
<li>
<p>Other teams that run the test in CI suffer from the spurious failures and wasted time
that hitting flaky tests entails</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><a href="3-selective-testing.html" class="xref page">Selective Testing</a> can help mitigate this to some extent by
letting you avoid running unrelated tests, but it doesnât make the problem fully disappear.
For example, a downstream service may be triggered every time an upstream utility library
is changed, and if the tests are flaky and the service and library are owned by different
teams, you end up with the conflict described above.</p>
</div>
<div class="paragraph">
<p>What ends up happening is that <em>nobody</em> prioritizes fixing their flaky tests, because
that is the selfishly-optimal thing to do, but as a result <em>everyone</em>
suffers from <em>everyone elseâs</em> flaky tests, even if everyone would be better if all flaky tests
were fixed. This is a classic <a href="https://en.wikipedia.org/wiki/Tragedy_of_the_commons">Tragedy of the Commons</a>,
and as long as flaky tests are allowed to exist, this will result in
endless debates or arguments between teams about who needs to fix their flaky tests,
wasting enormous amounts of time and energy.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_mitigating_flaky_tests"><a class="anchor" href="#_mitigating_flaky_tests"></a>Mitigating Flaky Tests</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In general it is impossible to completely avoid flaky tests, but you can take steps to
mitigate them:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Avoid race conditions in your application code to prevent random crashes or behavioral changes
affecting users, and avoid race conditions in your test code</p>
</li>
<li>
<p>Run parallel test processes inside &quot;sandbox&quot; empty temp folders, to try and avoid
them reading and writing to the same files on the filesystem and risking race conditions.
(See <a href="../mill/depth/sandboxing.html" class="xref page">Mill Sandboxing</a>)</p>
</li>
<li>
<p>Run test processes inside CGroups to mitigate resource contention: e.g. if every test process is limited
in how much memory it uses, it cannot cause memory pressure that might cause other tests
to be OOM-killed (See Bazelâs <a href="https://github.com/bazelbuild/bazel/pull/21322">Extended CGroup Support</a>,
which we implemented in <a href="https://www.databricks.com/blog/2021/10/14/developing-databricks-runbot-ci-solution.html">Databricks' Runbot CI system</a>)</p>
</li>
<li>
<p>Mock out external services: e.g. AWS and Azure can be mocked using <a href="https://www.localstack.cloud/">LocalStack</a>, parts of Azure
Kubernetes can be mocked using <a href="https://kind.sigs.k8s.io/">KIND</a>, etc..</p>
</li>
<li>
<p><a href="3-selective-testing.html" class="xref page">Selective Testing</a>, e.g. via
<a href="../mill/large/selective-execution.html" class="xref page">Millâs Selective Test Execution</a>, reduces the
number of tests you run
and thus  the impact of flakiness,</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>However, although you can mitigate the flakiness, you should not expect to make it go away
entirely.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Race conditions <em>will</em> find their way into your code despite your best efforts, and you <em>will</em>
need some hardcoded timeouts to prevent your test suite hanging forever.</p>
</li>
<li>
<p>There will always be <em>some</em> limited physical resource you didnât realize could run out,
until it does.</p>
</li>
<li>
<p>Mocking out third-party services never ends up working 100%: inevitably
you hit cases where the mock isnât accurate enough, or trustworthy enough, and you still
need to test against the real service to get confidence in the correctness of your system.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>End-to-end tests and integration tests are especially prone to flakiness, as are UI
tests exercising web or mobile app UIs.</p>
</div>
<div class="paragraph">
<p>As a developer, you should work hard in trying to make your application and test
code as deterministic as possible. You should have a properly-shaped
<a href="https://martinfowler.com/articles/practical-test-pyramid.html">Test Pyramid</a>, with more small unit
tests that tend to be stable and fewer integration/end-to-end/UI tests that tend to be flaky.
But you should also accept that despite your best efforts, flaky tests <em>will</em> appear, and so you
will need some plan or strategy to deal with them when they do.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_how_not_to_manage_flaky_tests"><a class="anchor" href="#_how_not_to_manage_flaky_tests"></a>How Not To Manage Flaky Tests</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Flaky test management can be surprisingly counter-intuitive. Below we discuss some common
mistakes people make when they first start dealing with flaky tests.</p>
</div>
<div class="sect2">
<h3 id="_do_not_block_code_changes_on_flaky_tests"><a class="anchor" href="#_do_not_block_code_changes_on_flaky_tests"></a>Do Not Block Code Changes on Flaky Tests</h3>
<div class="paragraph">
<p>The most important thing to take note of is that you should not block
code changes on flaky tests: merging pull-requests, deploying services, etc.</p>
</div>
<div class="paragraph">
<p>That is despite blocking code changes being the default and most obvious behavior: e.g.
if you wait for a fully-green test run before merging a code change, and a flaky test
makes the test run red, then it blocks the merge.However, this is not a good workflow
for a variety of reasons:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>A flaky failure when testing a code change does not indicate the code change caused
that breakage.So blocking the merge on the flaky failure just prevents progress
without actually helping increase system quality.</p>
</li>
<li>
<p>The flaky test may be in a part of the system totally unrelated to the code change
being tested, which means the individual working on the code change has zero context
on why it might be flaky, and unexpectedly context switching to deal with the flaky test
is mentally costly.</p>
</li>
<li>
<p>Blocking progress on a flaky test introduces an incentives problem: The code/test owner
benefits from the flaky testâs existence, but other people working in that codebase
get blocked with no benefit. This directly leads to the endless <a href="#_inter_team_conflict">Inter-team Conflict</a>
mentioned earlier.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Although <em>&quot;all tests should pass before merging&quot;</em> is a common requirement, it is ultimately
unhelpful when you are dealing with flaky tests.</p>
</div>
</div>
<div class="sect2">
<h3 id="_preventing_flaky_tests_from_being_introduced_is_hard"><a class="anchor" href="#_preventing_flaky_tests_from_being_introduced_is_hard"></a>Preventing Flaky Tests From Being Introduced Is Hard</h3>
<div class="paragraph">
<p>It can be tempting to try and &quot;<a href="https://en.wikipedia.org/wiki/Shift-left_testing">Shift Left</a>&quot;
your flaky test management, to try and catch them before they end up landing in your codebase.
But doing so ends up being surprisingly difficult.</p>
</div>
<div class="paragraph">
<p>Consider the example we used earlier: 10,000 tests, with 1% of them flaky, each failing 1% of
the time. These are arbitrary numbers but pretty representative of what you will likely find
in the wild</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>If someone adds a new test case, in order to have a 95% confidence that it is not flaky,
you would need to run it about 300 times (<code>log(0.05) / log(0.99)</code>).</p>
</li>
<li>
<p>Even if we do run every new test 300 times, that 1 in 20 flaky tests will still slip through,
and over time will still build up into a population of flaky tests actively causing flakiness
in your test suite</p>
</li>
<li>
<p>Furthermore, many tests are not flaky alone! Running the same test 300 times in
isolation may not demonstrate any flakiness, since e.g. the test may only be flaky when
run in parallel with another test due to <a href="#_race_conditions_between_tests">Race conditions between tests</a> or <a href="#_resource_contention">Resource contention</a>,
or in a specific order after other tests due to <a href="#_test_ordering_dependencies">Test ordering dependencies</a>.</p>
</li>
<li>
<p>Lastly, it is not only new tests that are flaky! When I was working on this area at Dropbox
and Databricks, the majority of flaky tests we detected were existing tests that
were stable for days/weeks/months before turning flaky (presumably due to a code change
in the application code or test code). Blocking new tests that are flaky does nothing
to prevent the code changes causing old tests to become flaky!</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>To block code changes that cause either new and old tests from becoming flaky, we would need
to run every single test about 300 times on each pull request, to give us 95% confidence that
each 1% flaky test introduced by the code change would get caught. This is prohibitively
slow and expensive, causing a test suite that may take 5 minutes to run costing $1 to instead
take 25 hours to run costing $300.</p>
</div>
<div class="paragraph">
<p>In general, it is very hard to block flaky tests &quot;up front&quot;. You have to accept that
over time some parts of your test suite will become flaky, and then make plans on how
to respond and manage those flaky tests when they inevitably appear.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_managing_flaky_tests"><a class="anchor" href="#_managing_flaky_tests"></a>Managing Flaky Tests</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Once flaky tests start appearing in your test suite, you need to do something about them.
This generally involves (a) noticing that flaky tests exist, (b) identifying which tests
are flaky, and (c) mitigating those specific problematic test to prevent them from
causing pain to your developers.</p>
</div>
<div class="sect2">
<h3 id="_monitor_flaky_tests_asynchronously"><a class="anchor" href="#_monitor_flaky_tests_asynchronously"></a>Monitor Flaky Tests Asynchronously</h3>
<div class="paragraph">
<p>As mentioned earlier, <a href="#_preventing_flaky_tests_from_being_introduced_is_hard">Preventing Flaky Tests From Being Introduced Is Hard</a>.
Thus, you must assume that flaky tests <em>will</em> make their way into your test suite,
and monitor the flakiness when it occurs. This can be done in a variety of ways, for example:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Most CI systems allow manual retries, and developers usually retry tests they suspect are
flaky. If a test fails once then passes when retried on the same version of the code, it
was a flaky failure. This is the metric we used in
<a href="https://www.databricks.com/blog/2021/10/14/developing-databricks-runbot-ci-solution.html">Databricks' CI system</a>
to monitor the flaky test numbers.</p>
</li>
<li>
<p>Some CI systems or test frameworks have automatic retries: e.g. in <a href="https://github.com/dropbox/changes">Dropboxâs Changes CI system</a>
all tests were retried twice by default. If a test fails initially and then
passes on the retry, it is flaky: the fact that itâs non-deterministic means that
next time, it might fail initially and then fail on the retry!</p>
</li>
<li>
<p>Most CI systems run tests to validate code changes before merging, and then run tests
again to validate the code post-merge. Post-merge should &quot;always&quot; be green, but sometimes
suffers breakages or flakiness. If a test passes, fails, then passes on three consecutive
commit test runs post-merge, it is likely to be flaky. Breakages
tend to cause a string of consecutive test failures before being fixed or reverted, and
very rarely get noticed and dealt with immediately</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Notably, most test failures when validating code changes (e.g. on pull requests) are not useful
here: tests are <em>meant</em> to break when validating code changes in order to catch problems!
Hence the need for the slightly-roundabout ways above to determine what tests are flaky,
by looking for failures at times when you wouldnât expect failures to occur.</p>
</div>
<div class="paragraph">
<p>Once you have noticed a test is flaky, there are two main options: retries and quarantine</p>
</div>
</div>
<div class="sect2">
<h3 id="_retrying_flaky_tests"><a class="anchor" href="#_retrying_flaky_tests"></a>Retrying Flaky Tests</h3>
<div class="paragraph">
<p>Retries are always controversial. A common criticism is that they can mask real flakiness
in the system that can cause real problems to customers, which is true. However,
we already discussed why we <a href="#_do_not_block_code_changes_on_flaky_tests">should
not block code changes on flaky tests</a>, since doing so just causes pain while
not being an effective way of getting the flakiness fixed.</p>
</div>
<div class="paragraph">
<p>Furthermore, developers
are going to be manually retrying flaky tests anyway: whether by restarting the job
validating their pull request, or running the test manually on their laptop or devbox
to check if itâs truly broken. Thus, we should feel free to add automatic retries around
flaky tests to automate that tedious manual process.</p>
</div>
<div class="paragraph">
<p>Retrying flaky tests can be surprisingly effective. As mentioned earlier, even
infrequently flaky tests can cause issues, with a small subset of tests flaking
1% of the time being enough to block all progress. However, one retry
turns it into a 0.01% flaky test, and two retries turns it into a 0.0001% flaky test.
So even one or two retries is enough to make most flaky tests stable enough to not cause issues.</p>
</div>
<div class="paragraph">
<p>Retrying flaky tests has two weaknesses:</p>
</div>
<div class="sect3">
<h4 id="_retries_can_be_expensive_for_real_failures"><a class="anchor" href="#_retries_can_be_expensive_for_real_failures"></a>Retries can be expensive for real failures</h4>
<div class="paragraph">
<p>If you retry a test twice, that
means that an actually-failed test would run three times before giving up.
If you retry every test by default, and a code change breaks a large number of
them, running all those failing tests three times can be a significant performance
and latency penalty</p>
</div>
<div class="paragraph">
<p>To mitigate this, you should generally avoid &quot;blanket&quot; retries, and only add
retries around specific tests that you have detected as being flaky</p>
</div>
</div>
<div class="sect3">
<h4 id="_retries_may_not_work_if_not_coarse_grained_enough"><a class="anchor" href="#_retries_may_not_work_if_not_coarse_grained_enough"></a>Retries may not work if not coarse grained enough</h4>
<div class="paragraph">
<p>For example, if <code>test_a</code> fails
due to interference with <code>test_b</code> running concurrently, retrying <code>test_a</code>
immediately while <code>test_b</code> is still running will fail again. Or if the flakiness is
due to some bad state on the filesystem, the test may continue flaking until
it is run on a completely new machine with a clean filesystem.</p>
</div>
<div class="paragraph">
<p>This failure mode can be mitigated by retrying the failed tests only after the
entire test suite has completed, possibly on a clean test machine.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_auto_quarantining_flaky_tests"><a class="anchor" href="#_auto_quarantining_flaky_tests"></a>Auto-Quarantining Flaky Tests</h3>
<div class="paragraph">
<p>Quarantine involves detecting that a test is flaky, and simply not counting it when deciding
whether or not to accept a code change for merge or deployment.</p>
</div>
<div class="paragraph">
<p>This is much more aggressive than retrying flaky tests, as even real breakages will get
ignored for quarantined tests. You effectively lose the test coverage given by a particular
test for the period while it is quarantined. Only when someone eventually fixes the flaky test
can it be removed from quarantine and can begin running and blocking code changes again.</p>
</div>
<div class="paragraph">
<p>Quarantining is best automated, both to remove busy-work of finding/quarantining
flaky tests, and to avoid the inevitable back-and-forth between the people
quarantining the tests and the people whose tests are getting quarantined.</p>
</div>
<div class="sect3">
<h4 id="_why_quarantine"><a class="anchor" href="#_why_quarantine"></a>Why Quarantine?</h4>
<div class="paragraph">
<p>The obvious value of quarantining flaky tests is that it unblocks merging of code changes
by ignoring flaky tests that are probably not relevant. Quarantime basically automates what
people do manually in the presence of flaky tests anyway:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>When enough tests are flaky, eventually developers are going to start merging/deploying code
changes despite the failures being present, because getting a &quot;fully green&quot; test run is
impossible</p>
</li>
<li>
<p>When that happens, the developer is not going to be able to tell whether the failure
is flaky or real, so if a code change causes a real breakage in that test the
developer is likely going to not notice and merge/deploy it anyway!</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>So although naively it seems like quarantining flaky tests cost you test coverage, in
reality it costs you nothing and simply automates the loss of coverage that you are going
to suffer anyway. It simply saves a lot of manual effort in having your developers manually
deciding which test failures to ignore based on what tests they remember to be flaky, since
now the quarantine system remembers the flaky tests and ignores them on your behalf.</p>
</div>
</div>
<div class="sect3">
<h4 id="_why_quarantine_part_2"><a class="anchor" href="#_why_quarantine_part_2"></a>Why Quarantine? Part 2</h4>
<div class="paragraph">
<p>The non-obvious value of quarantining flaky tests is that it aligns incentives across a
development team or organization:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Normally, a flaky test meant the test owner continues to benefit from the test coverage while
other teams suffered from the flakiness</p>
</li>
<li>
<p>With auto-quarantine, a flaky test means the test owner both benefits from the test coverage for
health tests and suffers the lack of coverage caused by their flaky test being quarantined.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This aligning of incentives means that with auto-quarantine enabled, the normal
endless discussions and disputes about flaky tests tend to disappear. The test owner
can decide themselves how urgently they need to fix a quarantined flaky test, depending
on how crucial that test coverage is, or even if they should fix it at all! Other teams
are not affected by the quarantined flaky test, and do not care what the test owner ends
up deciding</p>
</div>
<div class="paragraph">
<p>Most commonly, quarantining is automatic, while un-quarantining a test can be automatic or manual.
Due to the non-determinstic nature of flakiness, itâs often hard to determine whether a flaky
test has been truly fixed or not, but it turns out it doesnât matter. If you try to fix a test,
take it out of quarantine, and it turns out to be still flaky, the auto-quarantine system will
just put it back into quarantine for you to take another look at it.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_implementing_flaky_test_management_systems"><a class="anchor" href="#_implementing_flaky_test_management_systems"></a>Implementing Flaky Test Management Systems</h2>
<div class="sectionbody">
<div class="paragraph">
<p>So far, all the discussion in this article has been at a high level. Exactly how to implement
it is left as an exercise to the reader, but is usually a mix of:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>retry{}</code> helpers in multiple languages you can sprinkle through your test code where necessary</p>
</li>
<li>
<p>A SQL database storing historical test results and retries</p>
</li>
<li>
<p>A SQL database or a text file committed in-repo to track quarantined tests</p>
</li>
<li>
<p>A service that looks at historical test results and retries and decides when/if to quarantine a test</p>
</li>
<li>
<p>Tweaks to your existing CI system to be able to work with all of the above: ignoring quarantined
tests, tracking retry counts, tracking test results, etc.</p>
</li>
<li>
<p>Some kind of web interface giving you visibility into all the components and workflows above,
so when things inevitably go wrong you are able to figure out whatâs misbehaving</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Usually flaky test management starts off as an entirely manual process, which works fine for small
projects. But as the size of the project grows, you inevitably need to augment the manual work
with some basic automation, and over time build out a fully automated system to do what you want.
So far I have not seen a popular out-of-the-box solution for this, and in my interviews with ~30
silicon valley companies it seems everyone ends up building their own. The
<a href="https://github.com/dropbox/changes">Dropbox CI System</a> and
<a href="https://www.databricks.com/blog/2021/10/14/developing-databricks-runbot-ci-solution.html">Databricks CI System</a>
I worked on both had their flaky test management bespoke and built in to the infrastructure.</p>
</div>
<div class="paragraph">
<p>None of the techniques discussed in this article are rocket science, and the challenge is mostly
just plumbing the necessary data back and forth between different parts of your CI system. But
hopefully this high-level discussion of how to manage flaky tests should give you a head start,
and save you the weeks or months it would take to learn the same
things that I have learned working on flaky tests over the past decade.</p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="5-executable-jars.html">How JVM Executable Assembly Jars Work</a></span>
  <span class="next"><a href="3-selective-testing.html">Faster CI with Selective Testing</a></span>
</nav>
</article>
  </div>
</main>
</div>
<script id="site-script" src="../_/js/site.js" data-ui-root-path="../_"></script>
<script async="async" src="../_/js/vendor/highlight.js"></script>
<script src="../_/js/vendor/lunr.js"></script>
<script src="../_/js/search-ui.js" id="search-ui-script" data-site-root-path=".." data-snippet-length="100" data-stylesheet="../_/css/search.css"></script>
<script async="async" src="../search-index.js"></script>
  
</body></html>