// tag::header[]

# Designing Automatic Parallelism for JVM Test Suites in Mill

:author: Li Haoyi
:revdate: ??? March 2025

_{author}, {revdate}_

Test suites are the ideal workload to parallelize, as they usually contain a large
number of independent tests that can run in parallel. But practically implementing
a parallelism strategy that works well on a wide range of workflows can be challenging:
parallelization can

This blog post will explore the design and evolution of the Mill build tool's test parallelism
strategy, from the initial incarnation as a simple serial test runner, to naive module-based and
class-based parallelism, to the dynamic parallelization strategy implemented in the latest
version of Mill 0.12.9. We will discuss the pros and cons of the different approaches to
test parallelization, analyze how they perform both theoretically and in practive,
and from that see how newer stratagies improve on areas where earlier stratagies fall short

// end::header[]


## Serial Execution

The Mill build tool started off without any parallelism by default, and that extended to
tests as well. When asked to execute tasks, Mill would:

1. Receive the build tasks specified at the command line
2. Do a https://en.wikipedia.org/wiki/Breadth-first_search[breadth first search] on the task graph to find the full list of transitive tasks
3. Sort the tasks in topological order using https://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm[Tarjan's Algorithm]
4. Execute the tasks in order one at a time, skipping those with earlier cached values we can re-use
5. If any tasks contained test suites, these would also be run one at a time in a single JVM subprocess


While this serial execution works fine, it's unsatisfying in a world of modern
computers each of which has anywhere from 8-16 CPU cores you can make use of. You may be
waiting seconds or minutes for your tests to run on 1 CPU core while the other 9 cores are sitting idle.

To evaluate how well _Serial Execution_ and later parallelism strategies worked in practice,
we performed two evaluations:

1. A theoretical analysis using a simplified example build
2. A practical benchmark using two real-world codebases with very different testing workloads

These benchmarks are very rough and ad-hoc, but nevertheless are detailed enough to give
a good understanding at the benefits and tradeoffs involved in the different parallelism
strategies.

### Theoretical Evaluation

To understand the concepts behind each strategy, we imagine using it to run the tests
on a simple example build with:

* 3 `test` modules
* With 2/4/6 test suites respectively
* A hypothetical computer with 3 CPUs to use

We can see this visualized below for serial execution:

```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0]
  splines=false
  subgraph cluster_a {
      label="ModuleA.test"
      style="dashed"
      TestClassA1 -> TestClassA2
      TestClassA1 [style=filled fillcolor=lightpink]
      TestClassA2 [style=filled fillcolor=lightpink]
  }


  subgraph cluster_b {
      label="ModuleB.test"
      style="dashed"
      TestClassB1 -> TestClassB2 -> TestClassB3 -> TestClassB4
      TestClassB1 [style=filled fillcolor=lightgreen]
      TestClassB2 [style=filled fillcolor=lightgreen]
      TestClassB3 [style=filled fillcolor=lightgreen]
      TestClassB4 [style=filled fillcolor=lightgreen]
  }

  subgraph cluster_c {
      label="ModuleC.test"
      style="dashed"
      TestClassC1 -> TestClassC2 -> TestClassC3 -> TestClassC4 -> TestClassC5 -> TestClassC6
      TestClassC1 [style=filled fillcolor=lightblue]
      TestClassC2 [style=filled fillcolor=lightblue]
      TestClassC3 [style=filled fillcolor=lightblue]
      TestClassC4 [style=filled fillcolor=lightblue]
      TestClassC5 [style=filled fillcolor=lightblue]
      TestClassC6 [style=filled fillcolor=lightblue]
  }

  TestClassA2 -> TestClassB1 [constraint=none]
  TestClassB4 -> TestClassC1 [constraint=none]

}
```

* The colored boxes represent _test classes_. This is a common unit of managing tests in
  the JVM ecosystem, and will serve as the base level of granularity for this article.

* The arrows represent the threads on which the test suites execute on, and in the case
  of _Serial Execution_ they all take place on a single thread.

* The dashed boxes represent the testrunner subprocesses that were spawned. In this case,
  every module's tests are run in a separate subprocess, for isolation.

There are two things we would like to minimize here:

1. The longest single-threaded path through the build, often called the https://en.wikipedia.org/wiki/Critical_path_method[Critical Path]
2. How many subprocesses the test suite spawns: subprocess spawning is expensive, especially
   JVM subprocesses that may take several seconds to launch and warm up to peak performance.

The _Serial Execution_, the numbers are:

|===
|                | *Serial Execution*
| Critical Path   | *12*
| # Of Subprocesses | *3*
|===

### Practical Evaluation

For the practical evaluation, we considered the test suites of two different codebases:

1. The subset of unit tests of the https://github.com/netty/netty[Netty networking framework],
   that run in the xref:mill:ROOT:comparisons/maven.adoc[Mill example Netty build].
   These contain a large number modules with a large number of test classes,
   but each test class runs relatively quickly (<1s). This kind of workload is often
   seen in library codebases where all logic and tests can take place quickly in memory.

2. The tests of Mill's own `scalalib` module. This is a single large module with a
   large number of test classes, but each test class runs relatively slowly (>10s). While
   not ideal, this kind of workload is not unusual in monolithic application codebases with
   heavy integration testing.

These two workloads are very different, and benefit from different characteristics in the 
parallel test runner: for fast unit tests minimizing the number of subprocesses spawned is
important, whereas for slower integrations test the subprocess overhead is small compared to
the time taken to run even a single test.

We will see how these numbers vary as we explore different testing strategies
below, but as a baseline the time taken for running these test suites under _Serial Execution_
is as follows

|===
|  | *Serial Execution*
| Netty unit tests | *28s*
| Mill scalalib tests | *502s*
|===

These results are run ad-hoc on my M1 Macbook Pro with 10 cores. The exact numbers will
vary based on what test suite you chose and where you run them, but the overall trends
and conclusions should be the same.


## Module Sharding

Mill has always task-level parallelism opt-in via the `-j`/`--jobs`
flag (the name taken from the https://en.wikipedia.org/wiki/Make_%28software%29[Make tool]),
and it became the default in Mill `0.12.0` to use
all cores on your system. During testing, typically each Mill module `foo` would
have a single `foo.test` sub-module associated with it with a single `foo.test.testForked` task, so
this task-level parallelism would effectively parallelize your test suites at a _module level_.

One consequence of this is that if your codebase was broken up into many small modules,
each module's tests could run in parallel. But if your codebase had a few large modules
you may not be able to really use all the CPU cores available on your machine.

Visualizing this on the theoretical example we saw earlier:

```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0]

  subgraph cluster_c {
      label="ModuleC.test"
      style="dashed"
      TestClassC1 -> TestClassC2 -> TestClassC3 -> TestClassC4 -> TestClassC5 -> TestClassC6
      TestClassC1 [style=filled fillcolor=lightblue]
      TestClassC2 [style=filled fillcolor=lightblue]
      TestClassC3 [style=filled fillcolor=lightblue]
      TestClassC4 [style=filled fillcolor=lightblue]
      TestClassC5 [style=filled fillcolor=lightblue]
      TestClassC6 [style=filled fillcolor=lightblue]
  }

  subgraph cluster_b {
      label="ModuleB.test"
      style="dashed"
      TestClassB1 -> TestClassB2 -> TestClassB3 -> TestClassB4
      TestClassB1 [style=filled fillcolor=lightgreen]
      TestClassB2 [style=filled fillcolor=lightgreen]
      TestClassB3 [style=filled fillcolor=lightgreen]
      TestClassB4 [style=filled fillcolor=lightgreen]
  }

  subgraph cluster_a {
      label="ModuleA.test"
      style="dashed"
      TestClassA1 -> TestClassA2
      TestClassA1 [style=filled fillcolor=lightpink]
      TestClassA2 [style=filled fillcolor=lightpink]
  }
}
```

We can see that because the three modules have different numbers of test classes
within them, `ModuleA.test` finishes first and that thread is idle until `ModuleB.test` and
`ModuleC.test`
finish later. While not ideal, this is a significant improvement over the serial case
in our theoretical example, shortening the critical path from 12 test suites to 6
while preserving the number of subprocesses spawned:


|===
| | Serial Execution | *Module Sharding*
| Critical Path   | 12 | *6*
| # Of Subprocesses | 3 | *3*
|===

The practical benchmarks also show significant improvements for the Netty unit tests,
running 3x faster as they can take full advantage of the parallel cores on my computer.
However the Mill scalalib tests show no significant speedup, as the benchmark is a single
large module that does not benefit from module-level parallelism.

|===
|  | *Serial Execution* |  *Module Sharding*
| Netty unit tests | 28s | *10s*
| Mill scalalib tests | 502s | *477s*
|===

While in theory it would be ideal to break up the Mill scalalib module into smaller modules
each with their own test suite, doing so is tedious and manual, and realistically often does
not happen as quickly as you might like. Thus a build tool testing strategy needs to be able
to handle these large monolithic modules and test suites in some reasonable manner.

## Static Sharding

To work around the limitations of module-level parallelism, Mill `0.12.0` introduced _static sharding_
via the `def testForkGrouping` flag. This allows the developer to take the `Seq[String]` containing
all the test class names and return a nested `Seq[Seq[String]]` with the original list broken down
into groups. Each test group would run in parallel in a separate JVM subprocess in a separate folder,
but within each group the tests would still run sequentially.

For example, the following configuration would take the list of test classes
and break it down into 1-element groups:

```scala
def testForkGrouping = discoveredTestClasses().grouped(1).toSeq
```

Using static test sharding, the execution of the test suites in our theoretical example now
looks like this, with each test class assigned its own subprocess, and those subprocesses
making full use of the three cores available in the example:

```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0]


  style="dashed"

  subgraph cluster_c1 { label=""; TestClassC1 [style=filled fillcolor=lightblue] }
  subgraph cluster_c2 { label=""; TestClassC2 [style=filled fillcolor=lightblue] }
  subgraph cluster_c3 { label=""; TestClassC3 [style=filled fillcolor=lightblue] }
  subgraph cluster_c4 { label=""; TestClassC4 [style=filled fillcolor=lightblue] }
  subgraph cluster_c5 { label=""; TestClassC5 [style=filled fillcolor=lightblue] }
  subgraph cluster_c6 { label=""; TestClassC6 [style=filled fillcolor=lightblue] }


  subgraph cluster_b1 { label=""; TestClassB1 [style=filled fillcolor=lightgreen] }
  subgraph cluster_b2 { label=""; TestClassB2 [style=filled fillcolor=lightgreen] }
  subgraph cluster_b3 { label=""; TestClassB3 [style=filled fillcolor=lightgreen] }
  subgraph cluster_b4 { label=""; TestClassB4 [style=filled fillcolor=lightgreen] }



  subgraph cluster_a1 { label=""; TestClassA1 [style=filled fillcolor=lightpink] }
  subgraph cluster_a2 { label=""; TestClassA2 [style=filled fillcolor=lightpink] }


  TestClassA1 -> TestClassB2 -> TestClassC1 -> TestClassC4
  TestClassA2 -> TestClassB3 -> TestClassC2 -> TestClassC5
  TestClassB1 -> TestClassB4 -> TestClassC3 -> TestClassC6
}
```

|===
| | Serial Execution | Module Sharding | *Static Sharding*
| Critical Path   | 12 | 6 | *4*
| # Of Subprocesses | 3 | 3 | *12*
|===

Here we have shortened the critical path further, from 6 test suites to just 4. However, it has
come at the cost of spawning significantly more JVM subprocesses, as each 1-testsuite group
is allocated its own process.

Our practical benchmarks reflect this change as well:

|===
| | Serial Execution | Module Sharding | *Static Sharding*
| Netty unit tests | 28s | 10s | *51s*
| Mill scalalib tests | 502s | 477s | *181s*
|===

* In the Netty unit test benchmark which have lots of short fast test suites, spawning a JVM for each test
  suite is very expensive. We see the time taken to run all tests ballooning from 10s to 51s, as
  any improvement in parallelism is dominated by the cost of spawning the additional JVMs

* For the Mill scalalib test benchmark which have suites that take 10s of seconds, spawning a JVM for
  each test is a much smaller cost. And so the increased parallelism is able to provide a 2-3x speedup

The basic problem with static test sharding is that the ideal sharding depends on the
runtime characteristics of your test suite, and will evolve over time as your test suite
evolves. Small, fast test classes would benefit from having a coarse-grained sharding
with many test classes per group. Large, slow test classes benefit from a fine-grained
sharding with only one test class per group. But figuring out the ideal sharding for
a given test suite can only be determined experimentally which is very tedious, and
keeping the sharding ideal as the test suite evolves is basically impossible.

Thus although group-based parallelism could serve as a reasonable band-aid for modules
with large numbers of slow tests, it could never be turned on by default.

## Dynamic Sharding

To try and solve this problem with static test sharding,
https://github.com/com-lihaoyi/mill/pull/4614[#4614] introduced a dynamic sharding approach
using a subprocess pool. The idea was that you never had more the `NUM_CPUS` tests running
in parallel anyway, so you could just spawn `NUM_CPUS` child processes and have that
fixed set of child processes pull tests off a queue and run them until the queue was empty.
This meant the JVM startup overhead was proportional to `O(NUM_CPUS)` rather than `O(NUM_TESTS)`,
a much smaller number resulting in much smaller JVM overhead overall.

If you consider this approach on our theoretical example, the execution looks something like this:

```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0]


  style="dashed"

  subgraph cluster_c1 {
    TestClassC1 [style=filled fillcolor=lightblue]
    TestClassC4 [style=filled fillcolor=lightblue]
  }


  subgraph cluster_c2 {
    TestClassC2 [style=filled fillcolor=lightblue]
    TestClassC5 [style=filled fillcolor=lightblue]
  }

  subgraph cluster_c3 {
    TestClassC3 [style=filled fillcolor=lightblue]
    TestClassC6 [style=filled fillcolor=lightblue]
  }


  subgraph cluster_b1 {
    TestClassB1 [style=filled fillcolor=lightgreen]
    TestClassB4 [style=filled fillcolor=lightgreen]
  }
  subgraph cluster_b2 {
    TestClassB2 [style=filled fillcolor=lightgreen]
  }
  subgraph cluster_b3 {
    TestClassB3 [style=filled fillcolor=lightgreen]
  }

  subgraph cluster_a1 {
    TestClassA1 [style=filled fillcolor=lightpink]
  }
  subgraph cluster_a2 {
    TestClassA2 [style=filled fillcolor=lightpink]
  }

  TestClassA1 -> TestClassB2 -> TestClassC1 -> TestClassC4
  TestClassA2 -> TestClassB3 -> TestClassC2 -> TestClassC5
  TestClassB1 -> TestClassB4 -> TestClassC3 -> TestClassC6
}
```

|===
| | Serial Execution | Module Sharding | Static Sharding | *Dynamic Sharding*
| Critical Path   | 12 | 6 | 4 | *4*
| # Of Subprocesses | 3 | 3 | 12 | *8*
|===

Above, you can see that first `TestClassA1`, `TestClassA2`, and `TestClassB1` are scheduled
and each assigned a subprocess. When `A1` and `A2` finish, new subprocesses need to be spawned
to run `B2` and `B3` (since they are running a different module's code and tests), but when
`B1` finishes the same process can run `B4`. Later, `C1`, `C2`,
and `C3` and share the subprocess with `C4`, `C5`, and `C6` respectively.

This sharing and re-use of subprocesses where possible is able to bring down the
number of subprocesses from 12 to 8 in our theoretical example, while preserving the
shortened critical path. However, 8 is still much more than the 3 subprocesses that
_serial execution_ or _module sharding_ needed, indicating that this approach does
still add significant overhead for spawning testrunners that the more naive approaches
do not.

This different in the number of subprocesses reflects in the practical benchmarks below:

|===
| | Serial Execution | Module Sharding | Static Sharding  | *Dynamic Sharding*
| Netty unit tests | 28s | 10s | 51s | *21s*
| Mill scalalib tests | 502s | 477s | 181s | *160s*
|===

In these numbers, we can see that both the Netty unit test benchmark and the Mill scalalib
benchmark show a significant speedup using dynamic sharding over static sharding. This can
be attributed to the reduced number of subprocesses that needed to be spawned. However,
the Netty unit test benchmark is still 2x slower than the more naive _module sharding_
approach.

From the diagram above, we can see the nature of the problem: although the dynamic
sharding approach can re-use the subprocesses where possible, the way it schedules
test classes does not optimize for re-use: ideally we would want `A1` and `A2` to
share one subprocess, `B1` `B2` `B3` `B4` to share another subprocess, etc. which
we don't see above.


## Biased Dynamic Sharding

The last piece of the puzzle was to use _dynamic test sharding_, but to bias the Mill
scheduler to running the _first_ child process as soon as possible, but _subsequent_
child processes only later if there were no other tasks to run.

Essentially, what biased dynamic sharding does is try to minimize the number of
child processes each module's test suite will run: it is better to have N modules
spawn 1 subprocess each that runs to completion, rather than having the N modules each take
turns spawning NUM_CPUS subprocesses to run its own tests in parallel before shutting down.
Biased dynamic sharding thus aims for the former, only allocating a module more child JVMs
if there are idle cores that are unused:

```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0]


  style="dashed"
  subgraph cluster_c1 {
    TestClassC1 [style=filled fillcolor=lightblue]
    TestClassC2 [style=filled fillcolor=lightblue]
    TestClassC3 [style=filled fillcolor=lightblue]
    TestClassC4 [style=filled fillcolor=lightblue]

  }
  subgraph cluster_c5 {
    TestClassC5 [style=filled fillcolor=lightblue]
    TestClassC6 [style=filled fillcolor=lightblue]
  }
  subgraph cluster_b1 {
    TestClassB1 [style=filled fillcolor=lightgreen]
    TestClassB2 [style=filled fillcolor=lightgreen]
    TestClassB3 [style=filled fillcolor=lightgreen]
    TestClassB4 [style=filled fillcolor=lightgreen]

  }




  subgraph cluster_a1 {
    TestClassA1 [style=filled fillcolor=lightpink]
    TestClassA2 [style=filled fillcolor=lightpink]

  }


  TestClassA1 -> TestClassA2 -> TestClassC5 -> TestClassC6
  TestClassB1 -> TestClassB2 -> TestClassB3 -> TestClassB4

  TestClassC1 -> TestClassC2 -> TestClassC3 -> TestClassC4
}
```

|===
| | Serial Execution | Module Sharding | Static Sharding | Dynamic Sharding | *Biased Dynamic Sharding*
| Critical Path   | 12 | 6 | 4 | 4 | *4*
| # Of Subprocesses | 3 | 3 | 12 | 8 | *4*
|===

In the synthetic example above, we can see that biased dynamic sharding is able 
to maintain the critical path at length 4, while reducing the number of subprocesses it
needs to spawn (dashed boxes) from 8 to 4. We can see that each of `ModuleA` `ModuleB` and `ModuleC`
is assigned a single subprocess to do all of its work, and only when there is a CPU core
(when `A1` and `A2` have completed) is `ModuleC` given the idle core to parallelize
its remaining test classes.

This is a strict improvement over the previous dynamic sharding and static sharding
approaches, and it is reflected in the practical benchmarks where both Netty unit
tests and Mill scalalib tests show speedups over the previous _dynamic sharding_ approach:

|===
| | Serial Execution | Module Sharding | Static Sharding | Dynamic Sharding | *Biased Dynamic Sharding*
| Netty unit tests | 28s | 10s | 51s | 21s | *12s*
| Mill scalalib tests | 502s | 477s | 181s | 160s | *132s*
|===

Notably, the Netty unit tests benchmark is now comparable to the performance we were 
seeing with module-level parallelism! Although there is still a slight slowdown in the
practical benchmark - presumably from the slight increase in the number of spawned JVMs
we see in the theoretical analysis - it is not longer a large 2-5x slowdown we see in
_static sharding_ and _dynamic sharding_. We have finally achieved a test parallelization
strategy that is flexible enough to handle widely varying workloads and providing
good performance without manual tuning, which is something prior attempts at
parallelizing test suites fell short at.


## Implementation

The implementation of the various parallelism strategies isn't complicated: the Mill
build tool is a JVM application, and all these strategies basically boil down to passing
``Runnable``s to a `ThreadPoolExecutor`, each one
using ``ProcessBuilder`` to spawn the test runner, and with different
granularity of the ``Runnable``s and different queues for the `ThreadPoolExecutor`
(e.g. _biased dynamic sharding_ using a `PriorityBlockingQueue` to bias the scheduler
to running the first test subprocess over others).

Perhaps the most interesting implementation detail is for dynamic sharding:
this requires the build tool to spawn a number of test runner subprocesses that
pull the test classes off of a queue to execute until all test classes have been
run. This is implemented using a folder on disk containing
one-file-per-test-class. Each of the test runner subprocesses simply loops over the
files in that folder and attempts to claim them via an
https://stackoverflow.com/questions/18706419/is-a-move-operation-in-unix-atomic[Atomic Filesystem Move].
This allows us to avoid the complexity of managing a third party queue system,
or dealing with RPCs between different processes via sockets or ``memmap``ed files.
The simple disk-based queue is also more than enough to handle the relatively
small-scale that the test runner operates at (100-1000s of test classes).


## Conclusion

It's interesting how similar the problem of parallelizing tests is to the challenge of
architecting any distributed system. In particular, the ideas of _static sharding_ and _dynamic
sharding_ should be familiar concepts to any backend or infrastructure engineer, and the same
tradeoffs that apply to their use in backend systems also apply to their use in a build tool's
test runner. Even the _biased dynamic sharding_ that Mill eventually settled on has equivalents
in e.g. https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/[Kubernetes' Pod Priorities]
which allow certain workloads to be prioritized over others on the shared cluster.

Mill's test parallelism strategy has gone through a lot of iterations and improvement
over the years, and traditionally it has always been a very finnicky process to tweak
the various `--jobs` or `testForkGrouping` configuration to try and get optimal
performance out of your test suites. Although configuration knobs still exist, and we
may continue to tweak the heuristics and defaults in future, Mill 0.12.9 with its
_biased dynamic sharding_ strategy is finally able to provide a good zero-config
strategy to parallelize your tests, reduce waiting time, and speed up the development
process.
