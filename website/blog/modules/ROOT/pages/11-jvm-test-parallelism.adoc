// tag::header[]

# Designing Automatic Parallelism for JVM Test Suites in Mill

:author: Li Haoyi
:revdate: ??? March 2025

_{author}, {revdate}_

Test suites are the ideal workload to parallelize, as they usually contain a large
number of independent tests that can run in parallel. But practically implementing
a parallelism strategy that works well on a wide range of workflows can be challenging.

This blog post will explore the design and evolution of the Mill build tool's test parallelism
strategy, from the initial incarnation as a simple serial test runner, to naive module-based and
class-based parallelism, to the dynamic parallelization strategy implemented in the latest
version of Mill 0.12.9. We will discuss the pros and cons of the different approaches to
test parallelization, analyze how they perform both theoretically and in practive,
and from that see how newer stratagies improve on areas where earlier stratagies fall short

// end::header[]


## Serial Execution

To begin with, Mill originally did not have any parallelism, and that extended to tests as well.
Mill would:

1. Receive the build tasks specified at the command line
2. Do a https://en.wikipedia.org/wiki/Breadth-first_search[breadth first search] on the task graph to find the full list of transitive tasks
3. Sort the tasks in topological order using https://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm[Tarjan's Algorithm]
4. Execute the tasks in order one at a time, except those that have a cached value from an earlier run we can re-use
5. If any tasks contained test suites, these would also be run one at a time in a single JVM subprocess


While this serial execution worked fine for many years, it's unsatisfying in a world of modern
computers each of which has anywhere from 8-16 CPU cores you can make use of: you may be
waiting seconds or minutes for your tests to run when your build tool is using only 1 of your
available CPU cores and the other 9 cores are sitting idle.

To evaluate how well <<Serial Execution>> and later parallelism strategies worked in practice,
we performed two evaluations:

1. A theoretical evaluation using a simplified example build
2. A practical evaluation using two real-world codebases with very different testing workloads

These benchmarks are very rough and ad-hoc, but nevertheless should give a good understanding
at the benefits and tradeoffs involved.

## Theoretical Evaluation

Below we visualize this on an example build with 3 `test` modules, with 2/4/6 test suites
respectively, on a hypothetical computer with 3 threads. The arrows represent the threads
on which the test suites execute on, and in the case of _Serial Execution_ they all take
place on a single thread.

```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0]
  splines=false
  subgraph cluster_a {
      label="ModuleA.test"
      style="dashed"
      TestClassA1 -> TestClassA2
      TestClassA1 [style=filled fillcolor=lightpink]
      TestClassA2 [style=filled fillcolor=lightpink]
  }


  subgraph cluster_b {
      label="ModuleB.test"
      style="dashed"
      TestClassB1 -> TestClassB2 -> TestClassB3 -> TestClassB4
      TestClassB1 [style=filled fillcolor=lightgreen]
      TestClassB2 [style=filled fillcolor=lightgreen]
      TestClassB3 [style=filled fillcolor=lightgreen]
      TestClassB4 [style=filled fillcolor=lightgreen]
  }

  subgraph cluster_c {
      label="ModuleC.test"
      style="dashed"
      TestClassC1 -> TestClassC2 -> TestClassC3 -> TestClassC4 -> TestClassC5 -> TestClassC6
      TestClassC1 [style=filled fillcolor=lightblue]
      TestClassC2 [style=filled fillcolor=lightblue]
      TestClassC3 [style=filled fillcolor=lightblue]
      TestClassC4 [style=filled fillcolor=lightblue]
      TestClassC5 [style=filled fillcolor=lightblue]
      TestClassC6 [style=filled fillcolor=lightblue]
  }

  TestClassA2 -> TestClassB1 [constraint=none]
  TestClassB4 -> TestClassC1 [constraint=none]

}
```

There are two things we would like to minimize here:

1. The longest single-threaded path through the build
2. How many JVM subprocesses the test suite spawns, because spawning JVM subprocesses are expensive

In this serial example, the numbers are:

|===
|                | *Serial Execution*
| Critical Path   | *12*
| # Of Subprocesses | *3*
|===

## Practical Evaluation

For the practical evaluation, we considered the test suites of two different codebases:

1. The unit tests of the https://github.com/netty/netty[Netty networking framework],
   or the subset that run in the xref:mill:ROOT:comparisons/maven.adoc[Mill example Netty build].
   These contain a large number modules with a large number of test classes,
   but each test class runs relatively quickly (<1s). This kind of worklaod is often
   seen in library codebases where all logic and tests can take place quickly in memory.

2. The unit tests of Mill's own `scalalib` module. This is a single large module with a 
   large number of test classes, but each test class runs relatively slowly (>10s). While
   not ideal, this kind of workload is not unusual in monolithic application codebases with
   heavy integration testing.

These two workloads are very different, and benefit from different characteristics in the 
parallel test runner, which we will see in detail as we explore different testing strategies
below. But as a baseline, the time taken for running these test suites under <<Serial Execution>>
is as follows

|===
|  | *Serial Execution*
| Netty Unit Tests | *28s*
| Mill scalalib Tests | *502s*
|===


## Module Sharding

Mill has always provided op-in task-level parallelism via the `-j`/`--jobs`
flag (the name taken from the Make build tool), and it became the default in Mill `0.12.0` to use
all cores on your system by default. During testing, typically each Mill module `foo` would
have a single `foo.test` sub-module associated with it with a single `foo.test.testForked`, so
Mill would effectively parallelize your test suites at a _module level_.

One consequence of this is that if your codebase was broken up into many small modules,
each module's tests could run in parallel. But if your codebase had a few large modules
you may not have enough parallelism to really use all the compute available on your machine.

Visualizing this on the theoretical example we saw earlier:

```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0]

  subgraph cluster_c {
      label="ModuleC.test"
      style="dashed"
      TestClassC1 -> TestClassC2 -> TestClassC3 -> TestClassC4 -> TestClassC5 -> TestClassC6
      TestClassC1 [style=filled fillcolor=lightblue]
      TestClassC2 [style=filled fillcolor=lightblue]
      TestClassC3 [style=filled fillcolor=lightblue]
      TestClassC4 [style=filled fillcolor=lightblue]
      TestClassC5 [style=filled fillcolor=lightblue]
      TestClassC6 [style=filled fillcolor=lightblue]
  }

  subgraph cluster_b {
      label="ModuleB.test"
      style="dashed"
      TestClassB1 -> TestClassB2 -> TestClassB3 -> TestClassB4
      TestClassB1 [style=filled fillcolor=lightgreen]
      TestClassB2 [style=filled fillcolor=lightgreen]
      TestClassB3 [style=filled fillcolor=lightgreen]
      TestClassB4 [style=filled fillcolor=lightgreen]
  }

  subgraph cluster_a {
      label="ModuleA.test"
      style="dashed"
      TestClassA1 -> TestClassA2
      TestClassA1 [style=filled fillcolor=lightpink]
      TestClassA2 [style=filled fillcolor=lightpink]
  }
}
```

We can see that because the three modules have different numbers of tests
within them, `ModuleA.test` finishes first and that thread is idle until `ModuleC.test`
finishes later. While not ideal, this is a significant improvement over the serial case
in for our theoretical example, shortening the critical path from 12 test suites to 6:


|===
| | Serial Execution | *Module-level Parallelism*
| Critical Path   | 12 | *6*
| # Of Subprocesses | 3 | *3*
|===

The practical benchmarks also show significant improvements for the Netty unit tests,
running 3x faster as they can take full advantage of the parallel cores on my computer.
However the Mill scalalib tests show no significant speedup, as the benchmark is a single
large module that does not benefit from module-level parallelism.

|===
|  | *Serial Execution* |  *Module-level Parallelism*
| Netty Unit Tests | 28s | *10s*
| Mill scalalib Tests | 502s | *477s*
|===

## Static Sharding

To work around the limitations of module-level parallelism, Mill `0.12.0` also introduced the
`def testForkGrouping` flag. This allows the developer to take the `Seq[String]` containing
all the test class names and return a `Seq[Seq[String]]` with the original list broken down
into groups, each of which would run in parallel in a separate JVM subprocess in a separate folder,
but within each process they would run sequentially.

For example, the following configuration would take the list of test classes
and break it down into 1-element groups:

```scala
def testForkGrouping = discoveredTestClasses().grouped(1).toSeq
```

Using static test sharding, the execution of the test suites in our theoretical example now
looks like this:

```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0]


  style="dashed"
  subgraph cluster_a1 { label=""; TestClassA1 [style=filled fillcolor=lightpink] }
  subgraph cluster_a2 { label=""; TestClassA2 [style=filled fillcolor=lightpink] }


  subgraph cluster_b1 { label=""; TestClassB1 [style=filled fillcolor=lightgreen] }
  subgraph cluster_b2 { label=""; TestClassB2 [style=filled fillcolor=lightgreen] }
  subgraph cluster_b3 { label=""; TestClassB3 [style=filled fillcolor=lightgreen] }
  subgraph cluster_b4 { label=""; TestClassB4 [style=filled fillcolor=lightgreen] }

  subgraph cluster_c1 { label=""; TestClassC1 [style=filled fillcolor=lightblue] }
  subgraph cluster_c2 { label=""; TestClassC2 [style=filled fillcolor=lightblue] }
  subgraph cluster_c3 { label=""; TestClassC3 [style=filled fillcolor=lightblue] }
  subgraph cluster_c4 { label=""; TestClassC4 [style=filled fillcolor=lightblue] }
  subgraph cluster_c5 { label=""; TestClassC5 [style=filled fillcolor=lightblue] }
  subgraph cluster_c6 { label=""; TestClassC6 [style=filled fillcolor=lightblue] }

  TestClassA1 -> TestClassA2 -> TestClassB1 -> TestClassB2
  TestClassB3 -> TestClassB4 -> TestClassC1 -> TestClassC2
  TestClassC3 -> TestClassC4 -> TestClassC5 -> TestClassC6
}
```

|===
| | Serial Execution | Module-level Parallelism | *Static Sharding*
| Critical Path   | 12 | 6 | *4*
| # Of Subprocesses | 3 | 3 | *12*
|===

Here we have shortened the critical path further, from 6 test suites to just 4. However, it has
come at the cost of spawning significantly more JVM subprocesses, as each 1-testsuite group
is allocated its own process.

Our practical benchmarks reflect this change as well:

|===
| | Serial Execution | Module-level Parallelism | *Static Sharding*
| Netty Unit Tests | 28s | 10s | *51s*
| Mill scalalib Tests | 502s | 477s | *181s*
|===

* In the Netty unit test benchmark which have lots of short fast test suites, spawning a JVM for each test
  suite is very expensive. We see the time taken to run all tests ballooning from 10s to 51s, as
  any improvement in parallelism is dominated by the cost of spawning the additional JVMs

* For the Mill scalalib test benchmark which have suites that take 10s of seconds, spawning a JVM for
  each test is a much smaller cost. And so the increased parallelism is able to provide a 2-3x speedup

Static test sharding is able to take a single large module with many test classes
and effectively parallelize it: during the initial rollout we found it could take Mill's own
`scalalib.test` suite and speed it up from ~5 minutes down to ~2 minutes: not quite the speedup
you would expect on my 10 core laptop, but a significant speedup nonetheless.

However, the problem with this approach is that it spawned a new JVM subprocess for every test
class. This overhead may be acceptable for slow heavyweight test classes (of which Mill's
`scalalib.test` was mostly made of), since the JVM overhead of 1-2 seconds of startup/warmup
is dwarfed by the test class taking 10-20 seconds to run. But for more lightweight test classes
that themselves only take a second to run, having 1-2 seconds of overhead is prohibitive.
For example, turning on `testForkGrouping` in the
xref:mill:ROOT:comparisons/maven.adoc[Mill example Netty build] _slows the test suite down_
from ~10s to to taking ~50s to run!

Thus although group-based parallelism could serve as a reasonable band-aid for modules
with large numbers of slow tests, it could never be turned on by default. Whether it sped
things up or slowed things down could only be determined experimentally on a case by case
basis.

## Dynamic Sharding

To try and solve this problem with static test sharding,
https://github.com/com-lihaoyi/mill/pull/4614[#4614] introduced a dynamic sharding approach
using a process pool. The idea was that you never had more the `NUM_CPUS` tests running
in parallel anyway, so you could just spawn `NUM_CPUS` child processes and have that
fixed set of child processes pull tests off a queue and run them until the queue was empty.
This meant the JVM startup overhead was proportional to `O(NUM_CPUS)` rather than `O(NUM_TESTS)`,
a much smaller number resulting in much smaller JVM overhead overall.

If you consider this approach on our theoretical example, the execution looks something like this:

```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0]


  style="dashed"
  subgraph cluster_a1 {
    TestClassA1 [style=filled fillcolor=lightpink]
  }
  subgraph cluster_a2 {
    TestClassA2 [style=filled fillcolor=lightpink]
  }

  subgraph cluster_b1 {
    TestClassB1 [style=filled fillcolor=lightgreen]
    TestClassB4 [style=filled fillcolor=lightgreen]
  }
  subgraph cluster_b2 {
    TestClassB2 [style=filled fillcolor=lightgreen]
  }
  subgraph cluster_b3 {
    TestClassB3 [style=filled fillcolor=lightgreen]
  }

  subgraph cluster_c1 {
    TestClassC1 [style=filled fillcolor=lightblue]
    TestClassC4 [style=filled fillcolor=lightblue]
  }


  subgraph cluster_c2 {
    TestClassC2 [style=filled fillcolor=lightblue]
    TestClassC5 [style=filled fillcolor=lightblue]
  }

  subgraph cluster_c3 {
    TestClassC3 [style=filled fillcolor=lightblue]
    TestClassC6 [style=filled fillcolor=lightblue]
  }


  TestClassA1 -> TestClassB2 -> TestClassC1 -> TestClassC4
  TestClassA2 -> TestClassB3 -> TestClassC2 -> TestClassC5
  TestClassB1 -> TestClassB4 -> TestClassC3 -> TestClassC6
}
```

|===
| | Serial Execution | Module-level Parallelism | Static Sharding | *Dynamic Sharding*
| Critical Path   | 12 | 6 | 4 | *4*
| # Of Subprocesses | 3 | 3 | 12 | *8*
|===

Although dynamic test sharding is able to bring down the number of JVMs from 12 to 8
while preserving the shortened critical path,
it is still much more than the 3 JVMs that serial execution or module-level parallelism
are able to provide. This also reflects in the practical benchmarks, which demonstrate
a significant speedup over static sharding, but for the Netty unit test benchmark with
many small tests the overhead from spawning JVMs mean it is still 2x slower than the
more naive module-level parallelism approach.

|===
| | Serial Execution | Module-level Parallelism | Static Sharding  | *Dynamic Sharding*
| Netty Unit Tests | 28s | 10s | 51s | *21s*
| Mill scalalib Tests | 502s | 477s | 181s | *160s*
|===


## Biased Dynamic Sharding

The last piece of the puzzle was to use dynamic test sharding, but to bias the Mill
scheduler to running the _first_ child process as soon as possible, but _subsequent_
child processes only later if there were no other tasks to run.

Essentially, what biased dynamic sharding does is try to minimize the number of
child processes each module's test suite will run: it is better to have N modules
spawn 1 JVM each that runs to completion, rather than having the N modules each take
turns spawning NUM_CPUS JVM's to run its own tests in parallel before shutting down.
Biased dynamic sharding thus aims for that, only allocating a module more child JVMs
if there are idle cores that are unused

```graphviz
digraph G {
  rankdir=LR
  node [shape=box width=0 height=0]


  style="dashed"
  subgraph cluster_a1 {
    TestClassA1 [style=filled fillcolor=lightpink]
    TestClassA2 [style=filled fillcolor=lightpink]

  }

  subgraph cluster_b1 {
    TestClassB1 [style=filled fillcolor=lightgreen]
    TestClassB2 [style=filled fillcolor=lightgreen]
    TestClassB3 [style=filled fillcolor=lightgreen]
    TestClassB4 [style=filled fillcolor=lightgreen]

  }

  subgraph cluster_c1 {
    TestClassC1 [style=filled fillcolor=lightblue]
    TestClassC2 [style=filled fillcolor=lightblue]
    TestClassC3 [style=filled fillcolor=lightblue]
    TestClassC4 [style=filled fillcolor=lightblue]

  }
  subgraph cluster_c5 {
    TestClassC5 [style=filled fillcolor=lightblue]
    TestClassC6 [style=filled fillcolor=lightblue]
  }



  TestClassA1 -> TestClassA2 -> TestClassC5 -> TestClassC6
  TestClassB1 -> TestClassB2 -> TestClassB3 -> TestClassB4

  TestClassC1 -> TestClassC2 -> TestClassC3 -> TestClassC4
}
```

|===
| | Serial Execution | Module-level Parallelism | Static Sharding | Dynamic Sharding | *Biased Dynamic Sharding*
| Critical Path   | 12 | 6 | 4 | 4 | *4*
| # Of Subprocesses | 3 | 3 | 12 | 8 | *4*
|===

In the synthetic example above, we can see that biased dynamic sharding is able 
to maintain the critical path at length 4, while reducing the number of JVMs it
needs to spawn from 8 to 4. This is a strict improvement over the previous
dynamic sharding and static sharding approaches, and it is reflected in the practical
benchmarks where both Netty unit tests and Mill scalalib tests show speedups:

|===
| | Serial Execution | Module-level Parallelism | Static Sharding | Dynamic Sharding | *Biased Dynamic Sharding*
| Netty Unit Tests | 28s | 10s | 51s | 21s | *12s*
| Mill scalalib Tests | 502s | 477s | 181s | 160s | *132s*
|===

Notably, the Netty unit tests benchmark is now comparable to the performance we were 
seeing with module-level parallelism. Although there is still a slight slowdown in the 
practical benchmark - presumably from the slight increase in the number of spawned JVMs
we see in the theoretical analysis - it is not longer a large 2-5x slowdown.

## Conclusion

Mill's test parallelism strategy has gone through a lot of iterations and improvement 
over the years, and traditionally it has always been a very finnicky process to tweak 
the various `--jobs` or `testForkGrouping` configuration to try and get optimal 
performance out of your test suites. But with the introduction of biased dynamic
sharding in Mill 0.12.9, Mill is finally able to provide a good zero-config parallelism
strategy that works across a wide range of workloads.

